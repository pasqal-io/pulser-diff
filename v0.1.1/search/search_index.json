{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":"<p><code>PulserDiff</code> is a differentiable backend for pulse-level quantum simulation framework <code>Pulser</code>.</p> <p>This package aims to provide completely differentiable quantum states/expectations through use of automatic differentiation capabilities of underlying <code>PyTorch</code> package.</p>"},{"location":"basic_usage/","title":"Basic usage","text":"<p>This notebook gives an overview of the <code>PulserDiff</code> package and introduces its features and possibilities. <code>PulserDiff</code> in its essence is a <code>PyTorch</code>-based differentiable backend for the Pulser framework. It allows for composing, simulating and executing pulse sequences for neutral-atom quantum devices. Moreover, the result of Schr\u00f6dinger equation solver can be differentiated with respect to sequence evaluation times $\\{t_i\\}_{i=1}^{N}$ or any parameters that define the constituent pulses of the sequence, e. g., amplitude $\\Omega$, detuning $\\delta$ or phase $\\phi$. This feature is facilitated by the auto-differentiation capabilities of the underlying <code>PyTorch</code> library.</p> <p>Differentiability of <code>PulserDiff</code>'s simulation result $\\psi(\\theta_i)$ with respect to various sequence parameters $\\theta_i$ also means that a user can implement optimization strategies based on minimization of an arbitrary user-defined loss function $l(\\theta_i)$. This opens the possibility to optimize pulse parameters in different contexts, such as finding the best parameters for some state preparation problem or the optimal pulse shaping that implement the desired gate with highest fidelity on a given quantum machine with specific hardware constraints.</p> <p>In the following sections we will present both the low-level features of <code>PulserDiff</code> showcasing how to calculate and visualize explicit derivatives and the higher-level optimization API for various pulse optimization tasks.</p> <p>First we import the necessary objects from <code>pulser</code> and <code>pyqtorch</code>.</p> In\u00a0[1]: Copied! <pre>import torch\nfrom torch import Tensor\n\nfrom typing import Callable\n\nfrom pulser import Sequence, Pulse, Register\nfrom pulser.devices import MockDevice\nfrom pulser.waveforms import BlackmanWaveform, RampWaveform, CustomWaveform, ConstantWaveform\n\nfrom pulser_diff.backend import TorchEmulator\nfrom pulser_diff.derivative import deriv_time, deriv_param\nfrom pulser_diff.utils import IMAT, ZMAT, kron\n\nimport matplotlib.pyplot as plt\nfrom scipy import interpolate\n\nfrom pyqtorch.utils import SolverType\n</pre> import torch from torch import Tensor  from typing import Callable  from pulser import Sequence, Pulse, Register from pulser.devices import MockDevice from pulser.waveforms import BlackmanWaveform, RampWaveform, CustomWaveform, ConstantWaveform  from pulser_diff.backend import TorchEmulator from pulser_diff.derivative import deriv_time, deriv_param from pulser_diff.utils import IMAT, ZMAT, kron  import matplotlib.pyplot as plt from scipy import interpolate  from pyqtorch.utils import SolverType <p>Since <code>PulserDiff</code> is a Pulser backend, the general workflow for defining a pulse sequence is identical. Thus, first we define the atom register. Note that the coordinates of qubits $(x_i, y_i)$ are differentiable parameters.</p> In\u00a0[2]: Copied! <pre># define coordinates of 4 qubits as torch tensors\n# requires_grad=True instructs the auto-differentiation engine that gradients will be stored for these tensors \n# that will allow derivatives with respect to these parameters be calculated\nq0_coords = torch.tensor([0.0, 0.0], requires_grad=True)\nq1_coords = torch.tensor([0.0, 8.0], requires_grad=True)\nq2_coords = torch.tensor([8.0, 0.0], requires_grad=True)\nq3_coords = torch.tensor([8.0, 8.0], requires_grad=True)\n\n# create register\nreg = Register({\"q0\": q0_coords, \"q1\": q1_coords, \"q2\": q2_coords, \"q3\": q3_coords})\nprint(reg)\n</pre> # define coordinates of 4 qubits as torch tensors # requires_grad=True instructs the auto-differentiation engine that gradients will be stored for these tensors  # that will allow derivatives with respect to these parameters be calculated q0_coords = torch.tensor([0.0, 0.0], requires_grad=True) q1_coords = torch.tensor([0.0, 8.0], requires_grad=True) q2_coords = torch.tensor([8.0, 0.0], requires_grad=True) q3_coords = torch.tensor([8.0, 8.0], requires_grad=True)  # create register reg = Register({\"q0\": q0_coords, \"q1\": q1_coords, \"q2\": q2_coords, \"q3\": q3_coords}) print(reg) <pre>Register({'q0': tensor([0., 0.], dtype=torch.float64, grad_fn=&lt;SelectBackward0&gt;), 'q1': tensor([0., 8.], dtype=torch.float64, grad_fn=&lt;SelectBackward0&gt;), 'q2': tensor([8., 0.], dtype=torch.float64, grad_fn=&lt;SelectBackward0&gt;), 'q3': tensor([8., 8.], dtype=torch.float64, grad_fn=&lt;SelectBackward0&gt;)})\n</pre> <p>Now we create an empty sequence, define channels and define the parameters of pulses that will be added to the sequence.</p> In\u00a0[3]: Copied! <pre># create sequence and declare channels\nseq = Sequence(reg, MockDevice)\nseq.declare_channel(\"rydberg_global\", \"rydberg_global\")\n\n# define pulse parameters\nomega = torch.tensor([5.0], requires_grad=True)\nstart_det = torch.tensor([-5.0], requires_grad=True)\narea = torch.tensor([torch.pi], requires_grad=True)\ndet = torch.tensor([0.0], requires_grad=True)\nphase = torch.tensor([0.0], requires_grad=True)\n\n# define time-dependent waveforms\namp_wf = BlackmanWaveform(800, area)\ndet_wf = RampWaveform(800, start_det, 0.0)\n\n# add pulses\nseq.add(Pulse(amp_wf, det_wf, 0), \"rydberg_global\")\nseq.add(Pulse.ConstantPulse(800, omega, det, phase), \"rydberg_global\")\n</pre> # create sequence and declare channels seq = Sequence(reg, MockDevice) seq.declare_channel(\"rydberg_global\", \"rydberg_global\")  # define pulse parameters omega = torch.tensor([5.0], requires_grad=True) start_det = torch.tensor([-5.0], requires_grad=True) area = torch.tensor([torch.pi], requires_grad=True) det = torch.tensor([0.0], requires_grad=True) phase = torch.tensor([0.0], requires_grad=True)  # define time-dependent waveforms amp_wf = BlackmanWaveform(800, area) det_wf = RampWaveform(800, start_det, 0.0)  # add pulses seq.add(Pulse(amp_wf, det_wf, 0), \"rydberg_global\") seq.add(Pulse.ConstantPulse(800, omega, det, phase), \"rydberg_global\") <p>Note that all pulse parameters created above as tensors with the <code>requires_grad=True</code> argument. However, if differentiation with respect to some of them is not necessary, it is beneficial to set <code>requires_grad=False</code> for memory saving purposes.</p> <p>When creating pulses, it is possible to mix <code>torch</code> tensors and simple <code>float</code> values as pulse parameters.</p> <p>Sequence simulation is performed using the standard Pulser interface. However, here we use the <code>torch</code>-based <code>TorchEmulator</code> class from <code>PulserDiff</code> to perform the numerical simulation instead of the <code>QutipEmulator</code> that uses the <code>qutip</code> package.</p> In\u00a0[4]: Copied! <pre>sim = TorchEmulator.from_sequence(seq, sampling_rate=0.1)\nresults = sim.run(time_grad=True, dist_grad=True, solver=SolverType.DP5_SE)\n</pre> sim = TorchEmulator.from_sequence(seq, sampling_rate=0.1) results = sim.run(time_grad=True, dist_grad=True, solver=SolverType.DP5_SE) <p>The <code>time_grad=True</code> argument instructs the simulation to store gradients for all evaluation times, thus allowing the calculation for output derivatives with respect to these times. Similarly, <code>dist_grad=True</code> enables the  calculation for derivatives with respect to inter-qubit distances $r_{ij}$ . We can also specify what solver from <code>pyqtorch</code> library to use for quantum dynamics simulation: <code>DP5_SE</code> - selects the Dormand-Prince-5 state vector solver, <code>KRYLOV_SE</code> selects the Krylov subspace method state vector solver.</p> <p>The <code>results</code> object contains the values of the system's wavefunction at times stored in the <code>sim.evaluation_times</code> tensor. Both these quantities are <code>torch</code> tensors.</p> In\u00a0[5]: Copied! <pre>print(\"Evaluation times:\")\nprint(sim.evaluation_times)\nprint()\nprint(\"Wavefunctions:\")\nprint(results.states)\n</pre> print(\"Evaluation times:\") print(sim.evaluation_times) print() print(\"Wavefunctions:\") print(results.states) <pre>Evaluation times:\ntensor([0.0000, 0.0100, 0.0200, 0.0300, 0.0400, 0.0500, 0.0600, 0.0700, 0.0800,\n        0.0900, 0.1000, 0.1100, 0.1200, 0.1300, 0.1400, 0.1500, 0.1610, 0.1710,\n        0.1810, 0.1910, 0.2010, 0.2110, 0.2210, 0.2310, 0.2410, 0.2510, 0.2610,\n        0.2710, 0.2810, 0.2910, 0.3010, 0.3110, 0.3220, 0.3320, 0.3420, 0.3520,\n        0.3620, 0.3720, 0.3820, 0.3920, 0.4020, 0.4120, 0.4220, 0.4320, 0.4420,\n        0.4520, 0.4620, 0.4720, 0.4830, 0.4930, 0.5030, 0.5130, 0.5230, 0.5330,\n        0.5430, 0.5530, 0.5630, 0.5730, 0.5830, 0.5930, 0.6030, 0.6130, 0.6230,\n        0.6330, 0.6440, 0.6540, 0.6640, 0.6740, 0.6840, 0.6940, 0.7040, 0.7140,\n        0.7240, 0.7340, 0.7440, 0.7540, 0.7640, 0.7740, 0.7840, 0.7940, 0.8050,\n        0.8150, 0.8250, 0.8350, 0.8450, 0.8550, 0.8650, 0.8750, 0.8850, 0.8950,\n        0.9050, 0.9150, 0.9250, 0.9350, 0.9450, 0.9550, 0.9660, 0.9760, 0.9860,\n        0.9960, 1.0060, 1.0160, 1.0260, 1.0360, 1.0460, 1.0560, 1.0660, 1.0760,\n        1.0860, 1.0960, 1.1060, 1.1160, 1.1270, 1.1370, 1.1470, 1.1570, 1.1670,\n        1.1770, 1.1870, 1.1970, 1.2070, 1.2170, 1.2270, 1.2370, 1.2470, 1.2570,\n        1.2670, 1.2770, 1.2880, 1.2980, 1.3080, 1.3180, 1.3280, 1.3380, 1.3480,\n        1.3580, 1.3680, 1.3780, 1.3880, 1.3980, 1.4080, 1.4180, 1.4280, 1.4380,\n        1.4490, 1.4590, 1.4690, 1.4790, 1.4890, 1.4990, 1.5090, 1.5190, 1.5290,\n        1.5390, 1.5490, 1.5590, 1.5690, 1.5790, 1.5890, 1.6000],\n       dtype=torch.float64, requires_grad=True)\n\nWavefunctions:\ntensor([[[ 0.0000e+00+0.0000e+00j],\n         [ 0.0000e+00+0.0000e+00j],\n         [ 0.0000e+00+0.0000e+00j],\n         ...,\n         [ 0.0000e+00+0.0000e+00j],\n         [ 0.0000e+00+0.0000e+00j],\n         [ 1.0000e+00+0.0000e+00j]],\n\n        [[ 2.8051e-20-7.5226e-21j],\n         [ 3.0453e-16+2.1964e-15j],\n         [ 3.0453e-16+2.1964e-15j],\n         ...,\n         [-2.1551e-07-1.3052e-05j],\n         [-2.1551e-07-1.3052e-05j],\n         [ 1.0000e+00+4.5088e-12j]],\n\n        [[ 3.4985e-17-1.5168e-17j],\n         [ 1.0538e-13+4.7094e-13j],\n         [ 1.0538e-13+4.7094e-13j],\n         ...,\n         [-2.1384e-06-7.8628e-05j],\n         [-2.1384e-06-7.8628e-05j],\n         [ 1.0000e+00+2.9262e-10j]],\n\n        ...,\n\n        [[ 8.7875e-03+8.3862e-03j],\n         [-3.4528e-02-3.8154e-02j],\n         [-3.4528e-02-3.8154e-02j],\n         ...,\n         [-4.1323e-02-1.3517e-01j],\n         [-4.1323e-02-1.3517e-01j],\n         [-7.4824e-01+2.6820e-01j]],\n\n        [[ 1.0735e-02+3.2853e-03j],\n         [-3.9119e-02-2.7055e-02j],\n         [-3.9119e-02-2.7055e-02j],\n         ...,\n         [-2.4931e-02-1.2133e-01j],\n         [-2.4931e-02-1.2133e-01j],\n         [-7.6107e-01+2.7151e-01j]],\n\n        [[ 8.8444e-03-2.1945e-03j],\n         [-3.8070e-02-1.4566e-02j],\n         [-3.8070e-02-1.4566e-02j],\n         ...,\n         [-6.3890e-03-1.0636e-01j],\n         [-6.3890e-03-1.0636e-01j],\n         [-7.7359e-01+2.7324e-01j]]], dtype=torch.complex128,\n       grad_fn=&lt;StackBackward0&gt;)\n</pre> <p>Using the <code>results</code> object allows to further calculate the necessary quantities, e. g., using the <code>expect</code> method to get expectation values at each evaluation time for some observable $\\hat{C}$.</p> In\u00a0[6]: Copied! <pre># create total magnetization observable\nn_qubits = len(reg._coords)\ntotal_magnetization = []\nfor i in range(n_qubits):\n    tprod = [IMAT for _ in range(n_qubits)]\n    tprod[i] = ZMAT\n    total_magnetization.append(kron(*tprod))\ntotal_magnetization = sum(total_magnetization)\n\n# calculate expectation values\nexp_val = results.expect([total_magnetization])[0].real\n</pre> # create total magnetization observable n_qubits = len(reg._coords) total_magnetization = [] for i in range(n_qubits):     tprod = [IMAT for _ in range(n_qubits)]     tprod[i] = ZMAT     total_magnetization.append(kron(*tprod)) total_magnetization = sum(total_magnetization)  # calculate expectation values exp_val = results.expect([total_magnetization])[0].real <p>The calculation for the time derivative for some expectation values is performed using the <code>deriv_time</code> function:</p> In\u00a0[7]: Copied! <pre># calculate time derivative\neval_times = sim.evaluation_times\npulse_endtimes = sim.endtimes\ngrad_time = deriv_time(f=exp_val, times=eval_times, pulse_endtimes=pulse_endtimes)\n</pre> # calculate time derivative eval_times = sim.evaluation_times pulse_endtimes = sim.endtimes grad_time = deriv_time(f=exp_val, times=eval_times, pulse_endtimes=pulse_endtimes) <p>The last argument <code>pulse_endtimes</code> to function <code>deriv_time</code> is used to provide the indices for the pulse start/end times in the <code>eval_times</code> tensor. This is needed to avoid possible spurious derivative behaviours at the boundaries between two ideal pulses.</p> <p>In order to check that the calculated time derivative is correct, we can create an interpolated function from expectation values using <code>scipy.interpolate</code> and find the corresponding derivative.</p> In\u00a0[8]: Copied! <pre># calculate the exact derivative with respect to time\nx = eval_times.detach().numpy()\ny = exp_val.detach().numpy()\ninterp_fx = interpolate.UnivariateSpline(x, y, k=5, s=0)\ndfdt_exact = interp_fx.derivative()(x)\n\n# visualize expectation values and time derivatives\nplt.plot(x, y, label=\"f(x)\")\nplt.scatter(x, grad_time.detach().numpy(), s=2, label=\"df/dt (auto-diff)\")\nplt.plot(x, dfdt_exact, label=\"df/dt (interpolated, exact)\")\nplt.legend()\n</pre> # calculate the exact derivative with respect to time x = eval_times.detach().numpy() y = exp_val.detach().numpy() interp_fx = interpolate.UnivariateSpline(x, y, k=5, s=0) dfdt_exact = interp_fx.derivative()(x)  # visualize expectation values and time derivatives plt.plot(x, y, label=\"f(x)\") plt.scatter(x, grad_time.detach().numpy(), s=2, label=\"df/dt (auto-diff)\") plt.plot(x, dfdt_exact, label=\"df/dt (interpolated, exact)\") plt.legend() Out[8]: <pre>&lt;matplotlib.legend.Legend at 0x7503246ce640&gt;</pre> <p>We can see that time derivative calculated using <code>torch</code> auto-differentiation engine closely match the exact derivative.</p> <p>The workflow is identical as when calculating the time derivative. Now, <code>PulserDiff</code> provides the <code>deriv_param</code> function.</p> <p>Let us calculate derivatives of expectation value with respect to amplitude <code>omega</code> of the second constant pulse of the sequence, to coordinates of qubit <code>q1</code> and inter-qubit distance <code>r</code> between qubits <code>q1</code> and <code>q3</code>. We can obtain all these derivative values by passing the necessary parameters to <code>deriv_param</code> in a single list.</p> In\u00a0[9]: Copied! <pre>exp_val\n</pre> exp_val Out[9]: <pre>tensor([-4.0000, -4.0000, -4.0000, -4.0000, -4.0000, -4.0000, -4.0000, -3.9999,\n        -3.9998, -3.9996, -3.9992, -3.9986, -3.9975, -3.9959, -3.9933, -3.9894,\n        -3.9831, -3.9746, -3.9629, -3.9468, -3.9254, -3.8971, -3.8606, -3.8141,\n        -3.7559, -3.6843, -3.5975, -3.4941, -3.3729, -3.2333, -3.0754, -2.9000,\n        -2.6886, -2.4832, -2.2696, -2.0530, -1.8390, -1.6338, -1.4434, -1.2732,\n        -1.1279, -1.0108, -0.9236, -0.8665, -0.8380, -0.8355, -0.8553, -0.8932,\n        -0.9504, -1.0120, -1.0789, -1.1482, -1.2174, -1.2850, -1.3497, -1.4111,\n        -1.4687, -1.5226, -1.5727, -1.6192, -1.6620, -1.7013, -1.7371, -1.7693,\n        -1.8005, -1.8251, -1.8465, -1.8647, -1.8800, -1.8925, -1.9025, -1.9103,\n        -1.9163, -1.9206, -1.9237, -1.9256, -1.9268, -1.9274, -1.9276, -1.9338,\n        -2.0003, -2.0664, -2.1265, -2.1816, -2.2326, -2.2803, -2.3253, -2.3679,\n        -2.4081, -2.4455, -2.4796, -2.5097, -2.5352, -2.5553, -2.5695, -2.5775,\n        -2.5792, -2.5745, -2.5642, -2.5489, -2.5292, -2.5054, -2.4781, -2.4473,\n        -2.4132, -2.3756, -2.3346, -2.2902, -2.2425, -2.1922, -2.1399, -2.0867,\n        -2.0284, -1.9769, -1.9276, -1.8810, -1.8371, -1.7952, -1.7542, -1.7128,\n        -1.6696, -1.6232, -1.5730, -1.5187, -1.4610, -1.4012, -1.3413, -1.2836,\n        -1.2255, -1.1801, -1.1435, -1.1171, -1.1014, -1.0967, -1.1028, -1.1193,\n        -1.1459, -1.1823, -1.2282, -1.2834, -1.3479, -1.4210, -1.5022, -1.5901,\n        -1.6926, -1.7889, -1.8859, -1.9812, -2.0729, -2.1594, -2.2399, -2.3140,\n        -2.3821, -2.4451, -2.5038, -2.5596, -2.6132, -2.6652, -2.7159, -2.7696],\n       dtype=torch.float64, grad_fn=&lt;SelectBackward0&gt;)</pre> In\u00a0[10]: Copied! <pre># create tensor of times (in ns) where derivative will be evaluated\ntimes = torch.linspace(0.0, 1600, 40)\n\n# we want to calculate derivative with respect to distance between qubits q0 and q3\nqq_distances = sim.qq_distances\nr = qq_distances[\"q0-q3\"]\n\n# calculate derivative values at each time\ndiff_params = [omega, q1_coords, r]\ngrads = []\nfor t in times:\n    grad_param = deriv_param(f=exp_val, x=diff_params, times=eval_times, t=t)\n    grads.append(torch.hstack(grad_param))\ngrads = torch.stack(grads)\n</pre> # create tensor of times (in ns) where derivative will be evaluated times = torch.linspace(0.0, 1600, 40)  # we want to calculate derivative with respect to distance between qubits q0 and q3 qq_distances = sim.qq_distances r = qq_distances[\"q0-q3\"]  # calculate derivative values at each time diff_params = [omega, q1_coords, r] grads = [] for t in times:     grad_param = deriv_param(f=exp_val, x=diff_params, times=eval_times, t=t)     grads.append(torch.hstack(grad_param)) grads = torch.stack(grads) <p>Similarly we can calculate derivatives with respect to qubit coordinates. Let's now plot some results.</p> In\u00a0[11]: Copied! <pre># visualize derivative with respect to amplitude\nplt.plot(x, y, label=\"f(x)\")\nplt.scatter(times.numpy()/1000, grads[:,0].numpy(), s=2, label=\"df/d(omega) (auto-diff)\")\nplt.legend()\n</pre> # visualize derivative with respect to amplitude plt.plot(x, y, label=\"f(x)\") plt.scatter(times.numpy()/1000, grads[:,0].numpy(), s=2, label=\"df/d(omega) (auto-diff)\") plt.legend() Out[11]: <pre>&lt;matplotlib.legend.Legend at 0x7503245d2d90&gt;</pre> In\u00a0[12]: Copied! <pre># visualize derivatives with respect to coordinates\nplt.plot(x, y, label=\"f(x)\")\nplt.scatter(times.numpy()/1000, grads[:,1].numpy(), s=2, label=\"df/d(x_coord) (auto-diff)\")\nplt.scatter(times.numpy()/1000, grads[:,2].numpy(), s=2, label=\"df/d(y_coord) (auto-diff)\")\nplt.legend()\n</pre> # visualize derivatives with respect to coordinates plt.plot(x, y, label=\"f(x)\") plt.scatter(times.numpy()/1000, grads[:,1].numpy(), s=2, label=\"df/d(x_coord) (auto-diff)\") plt.scatter(times.numpy()/1000, grads[:,2].numpy(), s=2, label=\"df/d(y_coord) (auto-diff)\") plt.legend() Out[12]: <pre>&lt;matplotlib.legend.Legend at 0x750324540d00&gt;</pre> In\u00a0[13]: Copied! <pre># visualize derivative with repsect to inter-qubit distance\nplt.plot(x, y, label=\"f(x)\")\nplt.scatter(times.numpy()/1000, grads[:, 3].numpy(), s=2, label=\"df/dr (auto-diff)\")\nplt.legend()\n</pre> # visualize derivative with repsect to inter-qubit distance plt.plot(x, y, label=\"f(x)\") plt.scatter(times.numpy()/1000, grads[:, 3].numpy(), s=2, label=\"df/dr (auto-diff)\") plt.legend() Out[13]: <pre>&lt;matplotlib.legend.Legend at 0x75032447d3d0&gt;</pre> <p>In the previous section, we demonstrated that <code>PulserDiff</code> is able to calculate first-order derivatives ${\\rm d}l(\\theta)/{\\rm d}\\theta_{i}$ for some output function $l(\\theta)$ with respect to various pulse and register parameters $\\theta_i$. <code>PulserDiff</code> can therefore be integrated in machine learning pipelines as a framework where a loss function is minimized using gradient-based approaches to optimize system parameters of choice. Here, the system is the pulse sequence, the loss function can be any function of the output of the state-vector simulator and the optimizable parameters are pulse, register and observable parameters. Let us now demonstrate how such optimization can be performed.</p> <p>The optimization API of <code>PulserDiff</code> is built around a <code>QuantumModel</code> class that enables the conversion of standard Pulser sequences into a <code>torch.nn.Module</code>. This enables the seamless integration into usual PyTorch ML workflows including optimizers and schedulers selection as well as optimization loop customization. Let us import the <code>QuantumModel</code> object.</p> In\u00a0[2]: Copied! <pre>from pulser_diff.model import QuantumModel\n</pre> from pulser_diff.model import QuantumModel <p>To showcase the basic workflow for sequence optimization, let's define first a toy problem. Let us assume that we need to find the parameters of a pulse sequence such that the expectation value $f(\\theta)=\\left\\langle \\psi(\\theta)\\right|\\hat{C}\\left|\\psi(\\theta)\\right\\rangle $ of an observable $\\hat{C}$ is equal to a predefined value, for instance $f_0=-0.5$. We can achieve this goal by optimizing any or all parameters from these two groups:</p> <ol> <li>Pulse parameters - amplitude $\\Omega$, detuning $\\delta$, phase $\\phi$, duration $\\tau_n$</li> <li>Register parameters - qubit coordinates $(x_i, y_i)$</li> </ol> Note: Pulse duration optimization is currently supported for sequences containing only constant pulses.  <p>We first define a sequence consisting of one constant and one time-dependent pulse and assume that we want to optimize the amplitude of the constant pulse and the area of the time-dependent one.</p> In\u00a0[3]: Copied! <pre># create register\nreg = Register.rectangle(1, 2, spacing=8, prefix=\"q\")\n\n# create sequence and declare channels\nseq = Sequence(reg, MockDevice)\nseq.declare_channel(\"rydberg_global\", \"rydberg_global\")\n\n# declare sequence variables\nomega_param = seq.declare_variable(\"omega\")\narea_param = seq.declare_variable(\"area\")\n\n# create pulses\npulse_const = Pulse.ConstantPulse(1000, omega_param, 0.0, 0.0)\namp_wf = BlackmanWaveform(800, area_param)\ndet_wf = RampWaveform(800, 5.0, 0.0)\npulse_td = Pulse(amp_wf, det_wf, 0)\n\n# add pulses\nseq.add(pulse_const, \"rydberg_global\")\nseq.add(pulse_td, \"rydberg_global\")\n</pre> # create register reg = Register.rectangle(1, 2, spacing=8, prefix=\"q\")  # create sequence and declare channels seq = Sequence(reg, MockDevice) seq.declare_channel(\"rydberg_global\", \"rydberg_global\")  # declare sequence variables omega_param = seq.declare_variable(\"omega\") area_param = seq.declare_variable(\"area\")  # create pulses pulse_const = Pulse.ConstantPulse(1000, omega_param, 0.0, 0.0) amp_wf = BlackmanWaveform(800, area_param) det_wf = RampWaveform(800, 5.0, 0.0) pulse_td = Pulse(amp_wf, det_wf, 0)  # add pulses seq.add(pulse_const, \"rydberg_global\") seq.add(pulse_td, \"rydberg_global\") <p>The created sequence is a standard Pulser parametrized sequence from declaring two variables <code>omega</code> for constant pulse's amplitude and <code>area</code> for Blackman pulse's area. <code>PulserDiff</code> utilizes Pulser's variable system to define optimizable sequence parameters. This is valid for any pulse parameter that supports Pulser's <code>Variable</code> object as an argument.</p> <p>To optimize this sequence we first have to wrap it in a <code>QuantumModel</code> object and supply initial values for <code>omega</code> and <code>area</code>.</p> In\u00a0[4]: Copied! <pre># define pulse parameters\nomega = torch.tensor([5.0], requires_grad=True)\narea = torch.tensor([torch.pi], requires_grad=True)\n\n# create quantum model from sequence\ntrainable_params = {\"omega\": omega, \"area\": area}\nconstraints = {\n    \"omega\": {\"min\": 4.5, \"max\": 5.5}\n}\nmodel = QuantumModel(seq, trainable_params, constraints=constraints, sampling_rate=0.5, solver=SolverType.KRYLOV_SE)\n\n# list trainable parameters of the model\nprint()\nfor name, param in model.named_parameters():\n    print(name)\n    print(param)\n    print('-------')\n</pre> # define pulse parameters omega = torch.tensor([5.0], requires_grad=True) area = torch.tensor([torch.pi], requires_grad=True)  # create quantum model from sequence trainable_params = {\"omega\": omega, \"area\": area} constraints = {     \"omega\": {\"min\": 4.5, \"max\": 5.5} } model = QuantumModel(seq, trainable_params, constraints=constraints, sampling_rate=0.5, solver=SolverType.KRYLOV_SE)  # list trainable parameters of the model print() for name, param in model.named_parameters():     print(name)     print(param)     print('-------') <pre>\nseq_param_values.area\nParameter containing:\ntensor([3.1416], requires_grad=True)\n-------\nseq_param_values.omega\nParameter containing:\ntensor([5.], requires_grad=True)\n-------\n</pre> <p>We can see from the code snippet above that initial values for the optimizable parameters are provided as <code>torch.Tensor</code> objects with <code>requires_grad=True</code>. This falls in line with the paradigm of PyTorch of creating a computational graph that facilitates gradient calculations with respect to leaf nodes, in our case the user-created <code>omega</code> and <code>area</code> tensors. Note that <code>trainable_params</code> dict key values must be identical to the Pulser sequence variables defined previously.</p> <p>An optional dict can be passed to the <code>QuantumModel</code> constructor with a supplementary <code>constraints</code> argument, that defines the min/max range of optimizable parameters. Here, we specified that the <code>omega</code> parameter value must not fall out of the range $[4.5, 5.5]$. Internally such constraints are enforced by using the <code>torch.clamp</code> function that allows gradient flow and simultaneously keeps the parameter value strictly in range.</p> <p>Finally, we print the optimizable parameters of the created <code>QuantumModel</code> instance to confirm that <code>omega</code> and <code>area</code> are indeed registered and will be visible to the PyTorch optimizer. The optimizer is defined in a standard way along with the loss function that in our case is the simple MSE loss.</p> In\u00a0[5]: Copied! <pre># define loss function and optimizer\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n</pre> # define loss function and optimizer loss_fn = torch.nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.05) <p>With loss function and optimizer in place we can now construct an optimization loop to solve the problem.</p> In\u00a0[6]: Copied! <pre>def run_opt_loop(model: QuantumModel, \n                 target_value: Tensor, \n                 optimizer: torch.optim.Optimizer,\n                 loss_fn: Callable,\n                 epochs: int = 100, \n) -&gt; None:\n    \n    # print initial expectation value as a result of simulating initial sequence\n    _, init_exp_val = model.expectation()\n    print(\"Initial expectation value:\", init_exp_val[-1])\n    print()\n\n    # optimize model parameters so that the final output expectation value matches the predefined value\n    for t in range(epochs):\n        # calculate prediction and loss\n        _, exp_val = model.expectation()\n        loss = loss_fn(exp_val.real[-1], target_value)\n\n        # standard backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        # enforce constraints on optimizable parameters\n        model.check_constraints()\n\n        print(f\"[{t}] loss: {loss:&gt;7f}\")\n\n        if loss &lt; 0.00001:\n            break\n\n        # update sequence with changed pulse parameter values\n        model.update_sequence()\n\n    # print expectation value with optimized model\n    _, init_exp_val = model.expectation()\n    print()\n    print(\"Optimized expectation value:\", init_exp_val[-1])\n    print()\n</pre> def run_opt_loop(model: QuantumModel,                   target_value: Tensor,                   optimizer: torch.optim.Optimizer,                  loss_fn: Callable,                  epochs: int = 100,  ) -&gt; None:          # print initial expectation value as a result of simulating initial sequence     _, init_exp_val = model.expectation()     print(\"Initial expectation value:\", init_exp_val[-1])     print()      # optimize model parameters so that the final output expectation value matches the predefined value     for t in range(epochs):         # calculate prediction and loss         _, exp_val = model.expectation()         loss = loss_fn(exp_val.real[-1], target_value)          # standard backpropagation         loss.backward()         optimizer.step()         optimizer.zero_grad()          # enforce constraints on optimizable parameters         model.check_constraints()          print(f\"[{t}] loss: {loss:&gt;7f}\")          if loss &lt; 0.00001:             break          # update sequence with changed pulse parameter values         model.update_sequence()      # print expectation value with optimized model     _, init_exp_val = model.expectation()     print()     print(\"Optimized expectation value:\", init_exp_val[-1])     print() In\u00a0[7]: Copied! <pre># run optimization loop\ntarget_value = torch.tensor(-0.5, dtype=torch.float64)\nrun_opt_loop(model, target_value, optimizer, loss_fn, 100)\n</pre> # run optimization loop target_value = torch.tensor(-0.5, dtype=torch.float64) run_opt_loop(model, target_value, optimizer, loss_fn, 100) <pre>Initial expectation value: tensor(-1.2763+0.j, dtype=torch.complex128, grad_fn=&lt;SelectBackward0&gt;)\n\n[0] loss: 0.602630\n[1] loss: 0.511404\n[2] loss: 0.416565\n[3] loss: 0.323062\n[4] loss: 0.235741\n[5] loss: 0.158972\n[6] loss: 0.096202\n[7] loss: 0.049484\n[8] loss: 0.019127\n[9] loss: 0.003641\n[10] loss: 0.000086\n[11] loss: 0.003037\n[12] loss: 0.005175\n[13] loss: 0.007339\n[14] loss: 0.009370\n[15] loss: 0.011185\n[16] loss: 0.012749\n[17] loss: 0.014054\n[18] loss: 0.015110\n[19] loss: 0.015934\n[20] loss: 0.016543\n[21] loss: 0.016714\n[22] loss: 0.016053\n[23] loss: 0.014692\n[24] loss: 0.012802\n[25] loss: 0.010579\n[26] loss: 0.008227\n[27] loss: 0.005940\n[28] loss: 0.003890\n[29] loss: 0.002212\n[30] loss: 0.000992\n[31] loss: 0.000264\n[32] loss: 0.000003\n\nOptimized expectation value: tensor(-0.4983+0.j, dtype=torch.complex128, grad_fn=&lt;SelectBackward0&gt;)\n\n</pre> <p>The optimization loop above follows the standard PyTorch rules with a couple of additional adjustments. Namely, we call the <code>check_constraints()</code> method of the <code>model</code> to ensure that all optimizable parameters fall in range with any given constraints. Then the <code>update_sequence()</code> method is called to reconstruct the underlying Pulser sequence with optimized values of parameters. This concludes a single optimization iteration and prepares for the next one.</p> <p>As we can see, after 32 iterations the loss decreased significantly and the target value of $-0.5$ was achieved with high accuracy. Printing the optimizable parameters of the model confirms that their values indeed changed.</p> In\u00a0[19]: Copied! <pre>print()\nfor name, param in model.named_parameters():\n    print(name, param)\n    print('-------')\n</pre> print() for name, param in model.named_parameters():     print(name, param)     print('-------') <pre>\nseq_param_values.area Parameter containing:\ntensor([2.5058], requires_grad=True)\n-------\nseq_param_values.omega Parameter containing:\ntensor([4.6157], requires_grad=True)\n-------\n</pre> <p>Finally, we can use the <code>built_seq</code> property of the <code>QuantumModel</code> instance and call the standard Pulser <code>draw()</code> method to visualize the optimized sequence.</p> In\u00a0[20]: Copied! <pre>model.built_seq.draw()\n</pre> model.built_seq.draw() <p>Let us say that for a different task we want to optimize the pulse amplitude <code>omega</code> and also the positions of the qubits in the register. To achieve this we need to instruct the <code>QuantumModel</code> to handle qubit coordinates as optimizable parameters. To do this, let us recreate the register by specifying explicit qubit coordinates and constructing a similar sequence.</p> In\u00a0[21]: Copied! <pre># create register\nq0_coords = torch.tensor([0.5, 0.4], requires_grad=True)\nq1_coords = torch.tensor([8.3, 0.1], requires_grad=True)\nreg = Register({\"q0\": q0_coords, \"q1\": q1_coords})\n\n# create sequence and declare channels\nseq = Sequence(reg, MockDevice)\nseq.declare_channel(\"rydberg_global\", \"rydberg_global\")\n\n# declare sequence variables\nomega_param = seq.declare_variable(\"omega\")\n\n# create pulses\npulse_const = Pulse.ConstantPulse(1000, omega_param, 0.0, 0.0)\namp_wf = BlackmanWaveform(800, 3.14)\ndet_wf = RampWaveform(800, 5.0, 0.0)\npulse_td = Pulse(amp_wf, det_wf, 0)\n\n# add pulses\nseq.add(pulse_const, \"rydberg_global\")\nseq.add(pulse_td, \"rydberg_global\")\n</pre> # create register q0_coords = torch.tensor([0.5, 0.4], requires_grad=True) q1_coords = torch.tensor([8.3, 0.1], requires_grad=True) reg = Register({\"q0\": q0_coords, \"q1\": q1_coords})  # create sequence and declare channels seq = Sequence(reg, MockDevice) seq.declare_channel(\"rydberg_global\", \"rydberg_global\")  # declare sequence variables omega_param = seq.declare_variable(\"omega\")  # create pulses pulse_const = Pulse.ConstantPulse(1000, omega_param, 0.0, 0.0) amp_wf = BlackmanWaveform(800, 3.14) det_wf = RampWaveform(800, 5.0, 0.0) pulse_td = Pulse(amp_wf, det_wf, 0)  # add pulses seq.add(pulse_const, \"rydberg_global\") seq.add(pulse_td, \"rydberg_global\") <p>Here we defined only a single sequence variable <code>omega</code> and specified qubit coordinates using <code>torch</code> tensors that require gradients. Setting <code>requires_grad=True</code> when creating qubit coordinates essentially serves the same purpose as declaring sequence variables with <code>declare_variable()</code> method. To instruct the <code>QuantumModel</code> to register qubit coordinates as optimizable parameters we must include them in the <code>trainable_params</code> dict as well.</p> In\u00a0[22]: Copied! <pre># define pulse parameters\nomega = torch.tensor([5.0], requires_grad=True)\n\n# create quantum model from sequence\ntrainable_params = {\n    \"omega\": omega, \n    \"q0\": q0_coords,\n    \"q1\": q1_coords,\n}\nconstraints = {\n    \"omega\": {\"min\": 4.5, \"max\": 5.5}\n}\nmodel = QuantumModel(seq, trainable_params, constraints=constraints, sampling_rate=0.5, solver=SolverType.KRYLOV_SE)\n\n# list trainable parameters of the model\nprint()\nfor name, param in model.named_parameters():\n    print(name)\n    print(param)\n    print('-------')\n</pre> # define pulse parameters omega = torch.tensor([5.0], requires_grad=True)  # create quantum model from sequence trainable_params = {     \"omega\": omega,      \"q0\": q0_coords,     \"q1\": q1_coords, } constraints = {     \"omega\": {\"min\": 4.5, \"max\": 5.5} } model = QuantumModel(seq, trainable_params, constraints=constraints, sampling_rate=0.5, solver=SolverType.KRYLOV_SE)  # list trainable parameters of the model print() for name, param in model.named_parameters():     print(name)     print(param)     print('-------') <pre>\nseq_param_values.omega\nParameter containing:\ntensor([5.], requires_grad=True)\n-------\nreg_param_values.q0\nParameter containing:\ntensor([0.5000, 0.4000], requires_grad=True)\n-------\nreg_param_values.q1\nParameter containing:\ntensor([8.3000, 0.1000], requires_grad=True)\n-------\n</pre> <p>Now we can run the usual optimization loop.</p> In\u00a0[23]: Copied! <pre># define loss function and optimizer\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n\n# run optimization loop\ntarget_value = torch.tensor(-0.5, dtype=torch.float64)\nrun_opt_loop(model, target_value, optimizer, loss_fn, 100)\n</pre> # define loss function and optimizer loss_fn = torch.nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.05)  # run optimization loop target_value = torch.tensor(-0.5, dtype=torch.float64) run_opt_loop(model, target_value, optimizer, loss_fn, 100) <pre>Initial expectation value: tensor(-1.4701+0.j, dtype=torch.complex128, grad_fn=&lt;SelectBackward0&gt;)\n\n[0] loss: 0.941186\n[1] loss: 0.740986\n[2] loss: 0.446993\n[3] loss: 0.141663\n[4] loss: 0.050000\n[5] loss: 0.023811\n[6] loss: 0.000827\n[7] loss: 0.070675\n[8] loss: 0.140152\n[9] loss: 0.142601\n[10] loss: 0.097798\n[11] loss: 0.039629\n[12] loss: 0.005480\n[13] loss: 0.000382\n[14] loss: 0.004977\n[15] loss: 0.008452\n[16] loss: 0.009640\n[17] loss: 0.009751\n[18] loss: 0.009751\n[19] loss: 0.010045\n[20] loss: 0.010649\n[21] loss: 0.011340\n[22] loss: 0.012268\n[23] loss: 0.014119\n[24] loss: 0.015349\n[25] loss: 0.015725\n[26] loss: 0.015227\n[27] loss: 0.014037\n[28] loss: 0.012451\n[29] loss: 0.010771\n[30] loss: 0.009224\n[31] loss: 0.007932\n[32] loss: 0.006925\n[33] loss: 0.006174\n[34] loss: 0.005628\n[35] loss: 0.005229\n[36] loss: 0.004927\n[37] loss: 0.004681\n[38] loss: 0.004460\n[39] loss: 0.004243\n[40] loss: 0.004016\n[41] loss: 0.003767\n[42] loss: 0.003490\n[43] loss: 0.003179\n[44] loss: 0.002830\n[45] loss: 0.002441\n[46] loss: 0.002017\n[47] loss: 0.001565\n[48] loss: 0.001106\n[49] loss: 0.000671\n[50] loss: 0.000308\n[51] loss: 0.000070\n[52] loss: 0.000002\n\nOptimized expectation value: tensor(-0.4987+0.j, dtype=torch.complex128, grad_fn=&lt;SelectBackward0&gt;)\n\n</pre> In\u00a0[24]: Copied! <pre># register/sequence parameters after optimization\nprint()\nfor name, param in model.named_parameters():\n    print(name, param)\n    print('-------')\n</pre> # register/sequence parameters after optimization print() for name, param in model.named_parameters():     print(name, param)     print('-------') <pre>\nseq_param_values.omega Parameter containing:\ntensor([4.5000], requires_grad=True)\n-------\nreg_param_values.q0 Parameter containing:\ntensor([0.2341, 0.4370], requires_grad=True)\n-------\nreg_param_values.q1 Parameter containing:\ntensor([8.5659, 0.0630], requires_grad=True)\n-------\n</pre> <p>In previous optimization examples, we assumed constant pulse durations. However <code>PulserDiff</code> allows optimizing these parameters as well. Note, that currently only constant-pulse sequences support duration optimization.</p> <p>Let us create a sequence containing 3 constant pulses first.</p> In\u00a0[25]: Copied! <pre># create register\nreg = Register.rectangle(1, 2, spacing=8, prefix=\"q\")\n\n# create sequence and declare channels\nseq = Sequence(reg, MockDevice)\nseq.declare_channel(\"rydberg_global\", \"rydberg_global\")\n\n# declare sequence variables\ndur1_param = seq.declare_variable(\"dur1\")\nomega_param = seq.declare_variable(\"omega\")\ndur2_param = seq.declare_variable(\"dur2\")\n\n# create pulses\npulse1 = Pulse.ConstantPulse(dur1_param, 2.0, 0.5, 0.0)\npulse2 = Pulse.ConstantPulse(400, omega_param, 0.0, 0.0)\npulse3 = Pulse.ConstantPulse(dur2_param, 3.0, 1.0, 0.0)\n\n# add pulses\nseq.add(pulse1, \"rydberg_global\")\nseq.add(pulse2, \"rydberg_global\")\nseq.add(pulse3, \"rydberg_global\")\n</pre> # create register reg = Register.rectangle(1, 2, spacing=8, prefix=\"q\")  # create sequence and declare channels seq = Sequence(reg, MockDevice) seq.declare_channel(\"rydberg_global\", \"rydberg_global\")  # declare sequence variables dur1_param = seq.declare_variable(\"dur1\") omega_param = seq.declare_variable(\"omega\") dur2_param = seq.declare_variable(\"dur2\")  # create pulses pulse1 = Pulse.ConstantPulse(dur1_param, 2.0, 0.5, 0.0) pulse2 = Pulse.ConstantPulse(400, omega_param, 0.0, 0.0) pulse3 = Pulse.ConstantPulse(dur2_param, 3.0, 1.0, 0.0)  # add pulses seq.add(pulse1, \"rydberg_global\") seq.add(pulse2, \"rydberg_global\") seq.add(pulse3, \"rydberg_global\") <p>Here the duration parameters are specified similarly as the amplitude - using Pulser's <code>declare_variable()</code> function. Now we can pass the created sequence to the <code>QuantumModel</code> constructor together with initial values for optimizable parameters.</p> In\u00a0[26]: Copied! <pre># define pulse parameters\nomega = torch.tensor([5.0], requires_grad=True)\ndur1 = torch.tensor([0.4], requires_grad=True)  # note that duration here is specified in us not ns\ndur2 = torch.tensor([0.2], requires_grad=True)\n\n# create quantum model from sequence\ntrainable_params = {\n    \"omega\": omega, \n    \"dur1\": dur1,\n    \"dur2\": dur2,\n}\nmodel = QuantumModel(seq, trainable_params, sampling_rate=0.5, solver=SolverType.KRYLOV_SE)\n\n# list trainable parameters of the model\nprint()\nfor name, param in model.named_parameters():\n    print(name)\n    print(param)\n    print('-------')\n</pre> # define pulse parameters omega = torch.tensor([5.0], requires_grad=True) dur1 = torch.tensor([0.4], requires_grad=True)  # note that duration here is specified in us not ns dur2 = torch.tensor([0.2], requires_grad=True)  # create quantum model from sequence trainable_params = {     \"omega\": omega,      \"dur1\": dur1,     \"dur2\": dur2, } model = QuantumModel(seq, trainable_params, sampling_rate=0.5, solver=SolverType.KRYLOV_SE)  # list trainable parameters of the model print() for name, param in model.named_parameters():     print(name)     print(param)     print('-------') <pre>\nseq_param_values.omega\nParameter containing:\ntensor([5.], requires_grad=True)\n-------\nseq_param_values.dur2\nParameter containing:\ntensor([0.2000], requires_grad=True)\n-------\nseq_param_values.dur1\nParameter containing:\ntensor([0.4000], requires_grad=True)\n-------\n</pre> <p>It is important to note that duration parameter must be specified in microseconds instead of nanoseconds as in Pulser. The reason for this is to have duration parameter values more or less on the same scale as other pulse characteristics for optimizer to be able to work with a single learning rate for all trainable parameters.</p> <p>Let us visualize the initial sequence.</p> In\u00a0[27]: Copied! <pre>model.built_seq.draw()\n</pre> model.built_seq.draw() <p>We can notice that even though the sequence seems to be created from perfect constant pulses with sharp edges, the actual shape of pulses is smoothed. This is due to the fact that <code>QuantumModel</code> internally converts the original sequence into a sequence consisting of a number of 1 ns constant pulses parametrized by variables of the original sequence. This produces smooth amplitude $f_{\\Omega,\\tau}(t)$, detuning $f_{\\delta,\\tau}(t)$ and phase $f_{\\phi,\\tau}(t)$ envelopes, to mimick the original sequence but supports differentiation with respect to pulse durations $\\tau$. This turns out to be possible by setting pulse durations as parameters for those envelopes functions.</p> <p>To run the optimization we again define the loss, optimizer and call the optimization loop funtion.</p> In\u00a0[28]: Copied! <pre># define loss function and optimizer\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n# run optimization loop\ntarget_value = torch.tensor(-0.5, dtype=torch.float64)\nrun_opt_loop(model, target_value, optimizer, loss_fn, 100)\n</pre> # define loss function and optimizer loss_fn = torch.nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # run optimization loop target_value = torch.tensor(-0.5, dtype=torch.float64) run_opt_loop(model, target_value, optimizer, loss_fn, 100) <pre>Initial expectation value: tensor(-1.0706+0.j, dtype=torch.complex128, grad_fn=&lt;SelectBackward0&gt;)\n\n[0] loss: 0.325592\n[1] loss: 0.245923\n[2] loss: 0.176822\n[3] loss: 0.119215\n[4] loss: 0.073794\n[5] loss: 0.040838\n[6] loss: 0.015497\n[7] loss: 0.003325\n[8] loss: 0.000037\n[9] loss: 0.002506\n[10] loss: 0.008470\n[11] loss: 0.014847\n[12] loss: 0.021034\n[13] loss: 0.024638\n[14] loss: 0.027541\n[15] loss: 0.026635\n[16] loss: 0.022155\n[17] loss: 0.019871\n[18] loss: 0.013906\n[19] loss: 0.010865\n[20] loss: 0.005966\n[21] loss: 0.003664\n[22] loss: 0.001322\n[23] loss: 0.000079\n[24] loss: 0.000028\n[25] loss: 0.000988\n[26] loss: 0.001777\n[27] loss: 0.003156\n[28] loss: 0.005335\n[29] loss: 0.006514\n[30] loss: 0.006911\n[31] loss: 0.007057\n[32] loss: 0.005859\n[33] loss: 0.005670\n[34] loss: 0.003641\n[35] loss: 0.002704\n[36] loss: 0.001292\n[37] loss: 0.000299\n[38] loss: 0.000001\n\nOptimized expectation value: tensor(-0.5012+0.j, dtype=torch.complex128, grad_fn=&lt;SelectBackward0&gt;)\n\n</pre> <p>It is also possible to specify different learning rate for the duration parameters to improve convergence speed. To achieve this we can create a list <code>opt_params</code> that will contain optimizable parameters together with custom learning rates, if necessary.</p> <pre><code># set different learning rate for duration parameter(s)\nopt_params = []\ndur_params = [\"dur1\", \"dur2\"]\nfor name, p in list(model1.named_parameters()):\n    d = {\"params\": p}\n    if name.split(\".\")[-1] in dur_params:\n        d[\"lr\"] = 0.01\n    opt_params.append(d)\n\n# create optimizer\noptimizer = torch.optim.Adam(opt_params, lr=0.1)\n</code></pre> <p>This optimizer will use <code>lr=0.01</code> for duration parameters and <code>lr=0.1</code> for all the rest of parameters.</p> In\u00a0[29]: Copied! <pre># register/sequence parameters after optimization\nprint()\nfor name, param in model.named_parameters():\n    print(name, param)\n    print('-------')\n</pre> # register/sequence parameters after optimization print() for name, param in model.named_parameters():     print(name, param)     print('-------') <pre>\nseq_param_values.omega Parameter containing:\ntensor([4.8984], requires_grad=True)\n-------\nseq_param_values.dur2 Parameter containing:\ntensor([0.1233], requires_grad=True)\n-------\nseq_param_values.dur1 Parameter containing:\ntensor([0.3284], requires_grad=True)\n-------\n</pre> In\u00a0[\u00a0]: Copied! <pre># optimized sequence visualization\nmodel.built_seq.draw()\n</pre> # optimized sequence visualization model.built_seq.draw() <p>Since the <code>built_seq</code> above is the internal representation with discretized pulses, one might want to build the original parameterized sequence with the obtained optimized parameters. This can be easily done by passing <code>named_parameters</code> of the model to the original parameterized sequence <code>seq</code>.</p> In\u00a0[\u00a0]: Copied! <pre>duration_params = [\"dur1\", \"dur2\"]\nfinal_param_values = {}\nfor name, value in model.named_parameters():\n    name = name.split(\".\")[-1]\n    if name in duration_params:\n        value = int(value.data*1000)\n    else:\n        value = value.data\n    final_param_values[name] = value\n\nseq.build(**final_param_values).draw()\n</pre> duration_params = [\"dur1\", \"dur2\"] final_param_values = {} for name, value in model.named_parameters():     name = name.split(\".\")[-1]     if name in duration_params:         value = int(value.data*1000)     else:         value = value.data     final_param_values[name] = value  seq.build(**final_param_values).draw() <p>In situations when finer-grained control over pulse shapes is required the existing Pulser's parametrized pulses with predefined shapes might not be sufficient. For such cases Pulser has <code>CustomWaveform</code> object where an arbitrary array of values describes the waveform. In <code>PulserDiff</code> this custom waveform can be used for similar purpose.</p> In\u00a0[3]: Copied! <pre># create register\nreg = Register.rectangle(1, 2, spacing=8, prefix=\"q\")\n\n# create sequence and declare channels\nseq = Sequence(reg, MockDevice)\nseq.declare_channel(\"rydberg_global\", \"rydberg_global\")\n\n# declare sequence variables for predefined pulses\nomega_param = seq.declare_variable(\"omega\")\narea_param = seq.declare_variable(\"area\")\n\n# create pulses with predefined shapes\npulse_const = Pulse.ConstantPulse(1000, omega_param, 0.0, 0.0)\namp_wf = BlackmanWaveform(800, area_param)\ndet_wf = RampWaveform(800, 5.0, 0.0)\npulse_td = Pulse(amp_wf, det_wf, 0)\n\n# create custom-shaped pulse\npulse_duration = 300\nomega_custom_param = seq.declare_variable(\"omega_custom\", size=pulse_duration)\ncust_amp = CustomWaveform(omega_custom_param)\ncust_det = ConstantWaveform(pulse_duration, 1.5)\npulse_custom = Pulse(cust_amp, cust_det, 0.0)\n\n# add pulses\nseq.add(pulse_const, \"rydberg_global\")\nseq.add(pulse_td, \"rydberg_global\")\nseq.add(pulse_custom, \"rydberg_global\")\n</pre> # create register reg = Register.rectangle(1, 2, spacing=8, prefix=\"q\")  # create sequence and declare channels seq = Sequence(reg, MockDevice) seq.declare_channel(\"rydberg_global\", \"rydberg_global\")  # declare sequence variables for predefined pulses omega_param = seq.declare_variable(\"omega\") area_param = seq.declare_variable(\"area\")  # create pulses with predefined shapes pulse_const = Pulse.ConstantPulse(1000, omega_param, 0.0, 0.0) amp_wf = BlackmanWaveform(800, area_param) det_wf = RampWaveform(800, 5.0, 0.0) pulse_td = Pulse(amp_wf, det_wf, 0)  # create custom-shaped pulse pulse_duration = 300 omega_custom_param = seq.declare_variable(\"omega_custom\", size=pulse_duration) cust_amp = CustomWaveform(omega_custom_param) cust_det = ConstantWaveform(pulse_duration, 1.5) pulse_custom = Pulse(cust_amp, cust_det, 0.0)  # add pulses seq.add(pulse_const, \"rydberg_global\") seq.add(pulse_td, \"rydberg_global\") seq.add(pulse_custom, \"rydberg_global\") <p>From the code above we can see that creation of the <code>CustomWaveform</code> is a bit different compared to predefined pulses. We pass the <code>CustomWaveform</code> constructor a single Pulser variable <code>omega_custom_param</code> that will hold all information about the pulse's shape. Since this information will be stored as an collection of numbers, the variable itself has size greater than 1, namely <code>size=pulse_duration</code>. Hence the duration of the <code>CustomWaveform</code> must be known in advance. Other than that creation of the custom pulse follows standard Pulser rules.</p> <p>Now we can define the pulse shape itself by creating a function $g_{\\theta}(t)$ in interval $[0, \\tau]$ that accepts an arbitrary number of parameters $\\{\\theta_1, \\theta_2,...\\}$ and where $\\tau$ is the duration of the pulse. Let us implement such a function consisting of sine and exponential functions.</p> In\u00a0[4]: Copied! <pre>def custom_wf(param1, param2):\n    x = torch.arange(pulse_duration) / pulse_duration\n    return param1 * torch.sin(torch.pi * x) * torch.exp(-param2 * x)\n</pre> def custom_wf(param1, param2):     x = torch.arange(pulse_duration) / pulse_duration     return param1 * torch.sin(torch.pi * x) * torch.exp(-param2 * x) <p><code>custom_wf</code> creates a tensor of size 300 filled uniformly with values from 0 to 1 and then applies the sine and exponential functions on it. This way the output of <code>custom_wf</code> is a 300-element tensor with values aranged according to the mathematical operations defined above. In this case the governing parameters are <code>param1</code> and <code>param2</code> that modify the shape of the waveform.</p> <p>Now we can create the <code>trainable_params</code> dict to denote parameters to optimize together with their corresponding initial values. <code>omega</code> and <code>area</code> parameters are specified as usual, however for the <code>omega_custom</code> parameter both governing variables <code>param1</code> and <code>param2</code> must be passed together with <code>custom_wf</code> function.</p> In\u00a0[5]: Copied! <pre># define pulse parameters\nomega = torch.tensor(5.0, requires_grad=True)\narea = torch.tensor(torch.pi, requires_grad=True)\nparam1 = torch.tensor(6.0, requires_grad=True)\nparam2 = torch.tensor(2.0, requires_grad=True)\n\n# create quantum model from sequence\ntrainable_params = {\n    \"omega\": omega, \n    \"area\": area,\n    \"omega_custom\": ((param1, param2), custom_wf)\n}\nmodel = QuantumModel(seq, trainable_params, sampling_rate=0.5, solver=SolverType.KRYLOV_SE)\n\n# list trainable parameters of the model\nprint()\nfor name, param in model.named_parameters():\n    print(name)\n    print(param)\n    print('-------')\n</pre> # define pulse parameters omega = torch.tensor(5.0, requires_grad=True) area = torch.tensor(torch.pi, requires_grad=True) param1 = torch.tensor(6.0, requires_grad=True) param2 = torch.tensor(2.0, requires_grad=True)  # create quantum model from sequence trainable_params = {     \"omega\": omega,      \"area\": area,     \"omega_custom\": ((param1, param2), custom_wf) } model = QuantumModel(seq, trainable_params, sampling_rate=0.5, solver=SolverType.KRYLOV_SE)  # list trainable parameters of the model print() for name, param in model.named_parameters():     print(name)     print(param)     print('-------') <pre>\nseq_param_values.omega\nParameter containing:\ntensor(5., requires_grad=True)\n-------\nseq_param_values.area\nParameter containing:\ntensor(3.1416, requires_grad=True)\n-------\ncall_param_values.omega_custom_0\nParameter containing:\ntensor(6., requires_grad=True)\n-------\ncall_param_values.omega_custom_1\nParameter containing:\ntensor(2., requires_grad=True)\n-------\n</pre> <p>It is evident from the printout above that the actual parameters registered in the <code>QuantumModel</code> are the <code>param1</code> and <code>param2</code> tensors. Here <code>omega_custom</code> acts as a placeholder that binds both optimizable parameters, respective waveform function and the corresponding duration of the custom pulse. Actual values for the custom waveform are recalculated internally in the <code>QuantumModel</code> during each optimization iteration.</p> <p>Visualization of the initial sequence confirms that the third pulse indeed has a custom shape.</p> In\u00a0[6]: Copied! <pre>model.built_seq.draw()\n</pre> model.built_seq.draw() <p>Optimization procedure is identical to all previous cases.</p> In\u00a0[8]: Copied! <pre># define loss function and optimizer\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\n\n# run optimization loop\ntarget_value = torch.tensor(-0.5, dtype=torch.float64)\nrun_opt_loop(model, target_value, optimizer, loss_fn, 100)\n</pre> # define loss function and optimizer loss_fn = torch.nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.1)   # run optimization loop target_value = torch.tensor(-0.5, dtype=torch.float64) run_opt_loop(model, target_value, optimizer, loss_fn, 100) <pre>Initial expectation value: tensor(-1.6479+0.j, dtype=torch.complex128, grad_fn=&lt;SelectBackward0&gt;)\n\n[0] loss: 1.317681\n[1] loss: 1.260685\n[2] loss: 1.211758\n[3] loss: 1.093589\n[4] loss: 0.904411\n[5] loss: 0.668549\n[6] loss: 0.423364\n[7] loss: 0.212152\n[8] loss: 0.069835\n[9] loss: 0.006809\n[10] loss: 0.004452\n[11] loss: 0.030921\n[12] loss: 0.061352\n[13] loss: 0.084028\n[14] loss: 0.096057\n[15] loss: 0.098251\n[16] loss: 0.092434\n[17] loss: 0.080537\n[18] loss: 0.064505\n[19] loss: 0.046430\n[20] loss: 0.028633\n[21] loss: 0.013579\n[22] loss: 0.003534\n[23] loss: 0.000001\n\nOptimized expectation value: tensor(-0.4992+0.j, dtype=torch.complex128, grad_fn=&lt;SelectBackward0&gt;)\n\n</pre> In\u00a0[9]: Copied! <pre># register/sequence parameters after optimization\nprint()\nfor name, param in model.named_parameters():\n    print(name, param)\n    print('-------')\n</pre> # register/sequence parameters after optimization print() for name, param in model.named_parameters():     print(name, param)     print('-------') <pre>\nseq_param_values.omega Parameter containing:\ntensor(4.2468, requires_grad=True)\n-------\nseq_param_values.area Parameter containing:\ntensor(1.3819, requires_grad=True)\n-------\ncall_param_values.omega_custom_0 Parameter containing:\ntensor(4.4225, requires_grad=True)\n-------\ncall_param_values.omega_custom_1 Parameter containing:\ntensor(3.5294, requires_grad=True)\n-------\n</pre> <p>In the previous sections sections we showed how one can optimize different parameters of the sequence pulses or the register, however we did not include any noise in the simulations. Noise is unavoidable in a realistic quantum system, thus <code>PulserDiff</code> allows to perform noisy simulations using the same framework as Pulseruses for such purpose.</p> <p>Let us revisit the toy example from section 2.1 and add some noise to that system.</p> In\u00a0[12]: Copied! <pre># create register\nreg = Register.rectangle(1, 2, spacing=8, prefix=\"q\")\n\n# create sequence and declare channels\nseq = Sequence(reg, MockDevice)\nseq.declare_channel(\"rydberg_global\", \"rydberg_global\")\n\n# declare sequence variables\nomega_param = seq.declare_variable(\"omega\")\narea_param = seq.declare_variable(\"area\")\n\n# create pulses\npulse_const = Pulse.ConstantPulse(1000, omega_param, 0.0, 0.0)\namp_wf = BlackmanWaveform(800, area_param)\ndet_wf = RampWaveform(800, 5.0, 0.0)\npulse_td = Pulse(amp_wf, det_wf, 0)\n\n# add pulses\nseq.add(pulse_const, \"rydberg_global\")\nseq.add(pulse_td, \"rydberg_global\")\n</pre> # create register reg = Register.rectangle(1, 2, spacing=8, prefix=\"q\")  # create sequence and declare channels seq = Sequence(reg, MockDevice) seq.declare_channel(\"rydberg_global\", \"rydberg_global\")  # declare sequence variables omega_param = seq.declare_variable(\"omega\") area_param = seq.declare_variable(\"area\")  # create pulses pulse_const = Pulse.ConstantPulse(1000, omega_param, 0.0, 0.0) amp_wf = BlackmanWaveform(800, area_param) det_wf = RampWaveform(800, 5.0, 0.0) pulse_td = Pulse(amp_wf, det_wf, 0)  # add pulses seq.add(pulse_const, \"rydberg_global\") seq.add(pulse_td, \"rydberg_global\") <p>The sequence and the register remain the same, however when creating the <code>QuantumModel</code> instance we pass an additional <code>SimConfig</code> object specifying the noise type that we want to add for the argument <code>noise_config</code>. In our case we want to simulate some dephasing noise. Since we are adding noise the state of the system no longer can be described using the wavefunction. Instead we must use the density matrix formalism with the dynamics of the system described by the Lindbland equation. Solver of this equation can be selected by passing <code>SolverType.DP5_ME</code> for the <code>solver</code> argument of the <code>QuantumModel</code>.</p> In\u00a0[13]: Copied! <pre>from pulser_diff.simconfig import SimConfig\n\n# define pulse parameters\nomega = torch.tensor([5.0], requires_grad=True)\narea = torch.tensor([torch.pi], requires_grad=True)\n\n# define trainable params and constraints\ntrainable_params = {\"omega\": omega, \"area\": area}\nconstraints = {\n    \"omega\": {\"min\": 4.5, \"max\": 5.5}\n}\n\n# define noise config\nnoise_config = SimConfig(noise=\"dephasing\", dephasing_rate=2.0)\n\n# create quantum model\nmodel = QuantumModel(seq, \n                     trainable_params, \n                     constraints=constraints, \n                     sampling_rate=0.5, \n                     solver=SolverType.DP5_ME, \n                     noise_config=noise_config)\n\n# list trainable parameters of the model\nprint()\nfor name, param in model.named_parameters():\n    print(name)\n    print(param)\n    print('-------')\n</pre> from pulser_diff.simconfig import SimConfig  # define pulse parameters omega = torch.tensor([5.0], requires_grad=True) area = torch.tensor([torch.pi], requires_grad=True)  # define trainable params and constraints trainable_params = {\"omega\": omega, \"area\": area} constraints = {     \"omega\": {\"min\": 4.5, \"max\": 5.5} }  # define noise config noise_config = SimConfig(noise=\"dephasing\", dephasing_rate=2.0)  # create quantum model model = QuantumModel(seq,                       trainable_params,                       constraints=constraints,                       sampling_rate=0.5,                       solver=SolverType.DP5_ME,                       noise_config=noise_config)  # list trainable parameters of the model print() for name, param in model.named_parameters():     print(name)     print(param)     print('-------') <pre>\nseq_param_values.omega\nParameter containing:\ntensor([5.], requires_grad=True)\n-------\nseq_param_values.area\nParameter containing:\ntensor([3.1416], requires_grad=True)\n-------\n</pre> <p>Having the newly created model we can follow the standard workflow and define the optimizer and run the optimization loop.</p> In\u00a0[14]: Copied! <pre># define loss function and optimizer\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n\n# run optimization loop\ntarget_value = torch.tensor(-0.5, dtype=torch.float64)\nrun_opt_loop(model, target_value, optimizer, loss_fn, 100)\n</pre> # define loss function and optimizer loss_fn = torch.nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.05)  # run optimization loop target_value = torch.tensor(-0.5, dtype=torch.float64) run_opt_loop(model, target_value, optimizer, loss_fn, 100) <pre>Initial expectation value: tensor(-0.3802+0.j, dtype=torch.complex128, grad_fn=&lt;SelectBackward0&gt;)\n\n[0] loss: 0.014346\n[1] loss: 0.013734\n[2] loss: 0.013086\n[3] loss: 0.012391\n[4] loss: 0.011650\n[5] loss: 0.010869\n[6] loss: 0.010056\n[7] loss: 0.009218\n[8] loss: 0.008369\n[9] loss: 0.007523\n[10] loss: 0.006692\n[11] loss: 0.006186\n[12] loss: 0.005712\n[13] loss: 0.005230\n[14] loss: 0.004742\n[15] loss: 0.004253\n[16] loss: 0.003764\n[17] loss: 0.003280\n[18] loss: 0.002806\n[19] loss: 0.002347\n[20] loss: 0.001911\n[21] loss: 0.001503\n[22] loss: 0.001131\n[23] loss: 0.000804\n[24] loss: 0.000527\n[25] loss: 0.000306\n[26] loss: 0.000145\n[27] loss: 0.000044\n[28] loss: 0.000002\n\nOptimized expectation value: tensor(-0.4985+0.j, dtype=torch.complex128, grad_fn=&lt;SelectBackward0&gt;)\n\n</pre> In\u00a0[15]: Copied! <pre>print()\nfor name, param in model.named_parameters():\n    print(name, param)\n    print('-------')\n\nmodel.built_seq.draw()\n</pre> print() for name, param in model.named_parameters():     print(name, param)     print('-------')  model.built_seq.draw() <pre>\nseq_param_values.omega Parameter containing:\ntensor([5.5000], requires_grad=True)\n-------\nseq_param_values.area Parameter containing:\ntensor([1.7127], requires_grad=True)\n-------\n</pre> <p>Notice that the optimized values of <code>omega=5.5</code> and <code>area=1.7127</code> are different from the ones obtained in section 2.1 with no noise where we had <code>omega=2.5058</code> and <code>area=4.6157</code>. This signals that the noise indeed alters the behavior of the system, hence different pulse parameter values are needed to solve the same problem.</p>"},{"location":"basic_usage/#introduction","title":"Introduction\u00b6","text":""},{"location":"basic_usage/#1-derivative-calculation","title":"1. Derivative calculation\u00b6","text":""},{"location":"basic_usage/#11-defining-the-sequence","title":"1.1 Defining the sequence\u00b6","text":""},{"location":"basic_usage/#12-calculating-the-time-derivative","title":"1.2 Calculating the time derivative\u00b6","text":""},{"location":"basic_usage/#13-calculating-derivatives-with-respect-to-pulse-or-register-parameters","title":"1.3 Calculating derivatives with respect to pulse or register parameters\u00b6","text":""},{"location":"basic_usage/#2-sequence-optimization","title":"2. Sequence optimization\u00b6","text":""},{"location":"basic_usage/#21-pulse-parameter-optimization","title":"2.1 Pulse parameter optimization\u00b6","text":""},{"location":"basic_usage/#22-register-parameter-optimization","title":"2.2 Register parameter optimization\u00b6","text":""},{"location":"basic_usage/#23-pulse-duration-optimization","title":"2.3 Pulse duration optimization\u00b6","text":""},{"location":"basic_usage/#24-pulse-shape-optimization","title":"2.4 Pulse shape optimization\u00b6","text":""},{"location":"basic_usage/#25-noisy-optimization","title":"2.5 Noisy optimization\u00b6","text":""},{"location":"gate_optimization/","title":"Gate optimization","text":"<p>The goal of this notebook is to use machine learning techniques to optimize pulse sequences for the emulation of digital gates on neutral-atom platforms with always-on interaction.</p> <p>For that purpose, we will use <code>PulserDiff</code>, a differentiable backend tailored for optimizing neutral-atom simulations.</p> <p>More precisely, we want to find three parameter functions: $\\Omega(t)$, $\\delta(t)$ and $\\phi(t)$ of the underlying neutral-atom Hamiltonian, such that its evolution in the interval $[0, T]$ is equivalent to the execution of some global rotation gate. For instance, here we target the global Hadamard gate $H^{\\otimes n}$.</p> <p>Since it is not feasible to optimize over an infinite dimensional set of functions, we need to restrict ourselves to a finite dimensional space $E$, and define a differentiable functions from $\\mathbb{R}^{n} \\rightarrow E$ to compute the gradient of the fidelity with respect to the parameters on $E$.</p> <p>First, let us import all necessary tools from Pulser and <code>PulserDiff</code> and implement a virtual device for the purposes of simulation.</p> In\u00a0[1]: Copied! <pre>import torch\n\nfrom torch import Tensor\nfrom pulser import Sequence, Pulse, Register \nfrom pulser.devices import VirtualDevice\nfrom pulser.waveforms import CustomWaveform\nfrom pulser.channels import Rydberg\nfrom pulser_diff.utils import trace, kron, interpolate_sine\n\nfrom pulser_diff.model import QuantumModel\nfrom pyqtorch.matrices import HMAT\n\nfrom pyqtorch.utils import SolverType\n\n\nMockDevice = VirtualDevice(\n    name=\"MockDevice\",\n    dimensions=2,\n    rydberg_level=60,\n    channel_objects=(\n        Rydberg.Global(12.566370614359172, 12.566370614359172, max_duration=None),\n    ),\n)\n</pre> import torch  from torch import Tensor from pulser import Sequence, Pulse, Register  from pulser.devices import VirtualDevice from pulser.waveforms import CustomWaveform from pulser.channels import Rydberg from pulser_diff.utils import trace, kron, interpolate_sine  from pulser_diff.model import QuantumModel from pyqtorch.matrices import HMAT  from pyqtorch.utils import SolverType   MockDevice = VirtualDevice(     name=\"MockDevice\",     dimensions=2,     rydberg_level=60,     channel_objects=(         Rydberg.Global(12.566370614359172, 12.566370614359172, max_duration=None),     ), ) <p>In this tutorial, we will first optimize constant-pulse sequences to retrieve results presented in the article \"Variational protocols for emulating digital gates using analog control with always-on interactions\".</p> <p>As a proof of concept, let us start with a simple 2-qubit register and draw it.</p> In\u00a0[2]: Copied! <pre>n_qubits = 2\ndist = torch.tensor([6.5], requires_grad=False)\nreg = Register.rectangle(1, n_qubits, spacing=dist)\nreg.draw()\n</pre> n_qubits = 2 dist = torch.tensor([6.5], requires_grad=False) reg = Register.rectangle(1, n_qubits, spacing=dist) reg.draw() <p>Next, we create the target Hadamard gate and define the gate infidelity function to serve as a minimizing loss.</p> In\u00a0[11]: Copied! <pre># The global Hadamard gate is the target gate for the optimization.\ntarget_gate = kron(*[HMAT for _ in range(n_qubits)])\n\ndef gate_infidelity(U: Tensor, V: Tensor) -&gt; Tensor:\n    dim = U.shape[0]\n    return 1 - (1 / dim) * abs(trace(torch.matmul(U.mH, V)))\n</pre> # The global Hadamard gate is the target gate for the optimization. target_gate = kron(*[HMAT for _ in range(n_qubits)])  def gate_infidelity(U: Tensor, V: Tensor) -&gt; Tensor:     dim = U.shape[0]     return 1 - (1 / dim) * abs(trace(torch.matmul(U.mH, V))) <p>Now we create an optimizable sequence from a collection of $N=8$ constant pulses with parametrized amplitude $\\Omega_i$, detuning $\\delta_i$ and phase $\\phi_i$ for $i \\in \\{0, 1, ..., N-1]$.</p> In\u00a0[12]: Copied! <pre>seq_duration = 1050\nn_pulses = 8\n\nseq = Sequence(reg, MockDevice)\nseq.declare_channel(\"rydberg_global\", \"rydberg_global\")\n\n# Declare sequence variables to be optimized as a dict.\nseq_vars = {}\nseq_vars[\"amp_params\"] = [seq.declare_variable(f\"amp_param_{i}\") for i in range(n_pulses)]\nseq_vars[\"det_params\"] = [seq.declare_variable(f\"det_param_{i}\") for i in range(n_pulses)]\nseq_vars[\"phase_params\"] = [seq.declare_variable(f\"phase_param_{i}\") for i in range(n_pulses)]\n\n# Add parameterized constant pulses to the sequence.\nfor i in range(n_pulses):\n    seq.add(\n        Pulse.ConstantPulse(\n            duration=seq_duration // n_pulses,\n            amplitude=seq_vars[\"amp_params\"][i], \n            detuning=seq_vars[\"det_params\"][i], \n            phase=seq_vars[\"phase_params\"][i]\n        ), \n        \"rydberg_global\"\n    )\n</pre> seq_duration = 1050 n_pulses = 8  seq = Sequence(reg, MockDevice) seq.declare_channel(\"rydberg_global\", \"rydberg_global\")  # Declare sequence variables to be optimized as a dict. seq_vars = {} seq_vars[\"amp_params\"] = [seq.declare_variable(f\"amp_param_{i}\") for i in range(n_pulses)] seq_vars[\"det_params\"] = [seq.declare_variable(f\"det_param_{i}\") for i in range(n_pulses)] seq_vars[\"phase_params\"] = [seq.declare_variable(f\"phase_param_{i}\") for i in range(n_pulses)]  # Add parameterized constant pulses to the sequence. for i in range(n_pulses):     seq.add(         Pulse.ConstantPulse(             duration=seq_duration // n_pulses,             amplitude=seq_vars[\"amp_params\"][i],              detuning=seq_vars[\"det_params\"][i],              phase=seq_vars[\"phase_params\"][i]         ),          \"rydberg_global\"     ) <p>For a more realistic simulation, we add device constraints for the range of allowed amplitude and detuning values. These values are provided by the <code>MockDevice</code> specification. Note, that in order to simulate the complete matrix representing the target quantum gate, we need to provide a custom batch of initial states for the <code>QuantumModel</code>. Each batch element contributes to a column in the final gate matrix.</p> In\u00a0[13]: Copied! <pre># Extract variable names for the optimizable parameters.\nvar_names = [var.var.name for var_list in seq_vars.values() for var in var_list]\n\n# Create a constraints dict (phase is unconstrained since it's related to the detuning).\nconstraints = {}\nfor name in var_names:\n    if \"amp\" in name:\n        # Apply device amplitude constraint.\n        constraints[name] = {\"min\": 0.0, \"max\": int(MockDevice.channels[\"rydberg_global\"].max_amp)}\n    if \"det\" in name:\n        # Apply device detuning constraint.\n        constraints[name] = {\"min\": -MockDevice.channels[\"rydberg_global\"].max_abs_detuning, \"max\": MockDevice.channels[\"rydberg_global\"].max_abs_detuning}\n\n# Set initial values for the optimizable parameters. Values are fixed for better convergence.\ntrainable_params = {name: torch.tensor(5.0) for name in var_names}\n\n# Create batch tensor of all possible initial states.\ninit_state = torch.eye(2 ** n_qubits)\n\nmodel = QuantumModel(\n    seq,\n    sampling_rate=0.05,\n    trainable_param_values=trainable_params,\n    constraints=constraints,\n    solver=SolverType.DP5_SE,\n    initial_state=init_state)\n</pre> # Extract variable names for the optimizable parameters. var_names = [var.var.name for var_list in seq_vars.values() for var in var_list]  # Create a constraints dict (phase is unconstrained since it's related to the detuning). constraints = {} for name in var_names:     if \"amp\" in name:         # Apply device amplitude constraint.         constraints[name] = {\"min\": 0.0, \"max\": int(MockDevice.channels[\"rydberg_global\"].max_amp)}     if \"det\" in name:         # Apply device detuning constraint.         constraints[name] = {\"min\": -MockDevice.channels[\"rydberg_global\"].max_abs_detuning, \"max\": MockDevice.channels[\"rydberg_global\"].max_abs_detuning}  # Set initial values for the optimizable parameters. Values are fixed for better convergence. trainable_params = {name: torch.tensor(5.0) for name in var_names}  # Create batch tensor of all possible initial states. init_state = torch.eye(2 ** n_qubits)  model = QuantumModel(     seq,     sampling_rate=0.05,     trainable_param_values=trainable_params,     constraints=constraints,     solver=SolverType.DP5_SE,     initial_state=init_state) <p>Finally, let's execute a simple optimization loop for 600 epochs or until the loss drops below 0.0009, for an optimized gate with 99.91% fidelity.</p> In\u00a0[14]: Copied! <pre># Define parameter values and initialize the optimizer and the scheduler.\ninitial_lr = 1.0\noptimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\nepochs = 1000\nmin_change = 0.01\nnum_loss_plateu = 6\n\nloss_dict = {}\nfor t in range(epochs):\n    # Calculate the loss with the final state.\n    _, gate = model.forward()\n    loss = gate_infidelity(target_gate, gate[-1])  # we are interested in the final state infidelity\n    \n    # Backpropagation.\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n    # Log the loss value together with model params.\n    loss_dict[t] = {\"loss\": float(loss), \"params\": {name: param.data.clone().detach() for name, param in model.named_parameters()}}\n\n    if len(loss_dict) &gt; num_loss_plateu and loss &gt; 0.1:\n        last_losses = [loss_dict[i][\"loss\"] for i in range(t-num_loss_plateu, t + 1)]\n        diffs = [abs(last_losses[i] - last_losses[i - 1]) for i in range(-1, -num_loss_plateu-1, -1)]\n        if all(diff &lt; min_change for diff in diffs):\n            # Reset the learning rate to the initial value and recreate the scheduler.\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = initial_lr\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n        else:\n            # Update the learning rate.\n            scheduler.step()\n    else:\n        # Update the learning rate.\n        scheduler.step()\n\n    # Enforce constraints on optimizable parameters.\n    model.check_constraints()\n\n    if loss &lt; 0.0009:\n        print(f\"[t={t}]loss: {float(loss):&gt;7f}\")\n        break\n\n    # Update the sequence with changed pulse parameter values.\n    model.update_sequence()\n\n    if (t % 50 == 0) or (t == epochs-1):\n        # Print the learning rate.\n        lr = scheduler.get_last_lr()[0]\n        print(f\"Epoch {t:03}: Learning Rate = {lr:.6f}\")\n\n        print(f\"[t={t}]loss: {float(loss):&gt;7f}\")\n        print(\"**************************************\")\n\n# Get the best parameter set.\nsorted_losses = dict(sorted(loss_dict.items(), key=lambda x: x[1][\"loss\"]))\nbest_param_set = list(sorted_losses.values())[0][\"params\"]\nprint(f\"Best loss: {list(sorted_losses.values())[0]['loss']} after {list(sorted_losses.keys())[0]} epochs.\")\n</pre> # Define parameter values and initialize the optimizer and the scheduler. initial_lr = 1.0 optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr) scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50) epochs = 1000 min_change = 0.01 num_loss_plateu = 6  loss_dict = {} for t in range(epochs):     # Calculate the loss with the final state.     _, gate = model.forward()     loss = gate_infidelity(target_gate, gate[-1])  # we are interested in the final state infidelity          # Backpropagation.     loss.backward()     optimizer.step()     optimizer.zero_grad()      # Log the loss value together with model params.     loss_dict[t] = {\"loss\": float(loss), \"params\": {name: param.data.clone().detach() for name, param in model.named_parameters()}}      if len(loss_dict) &gt; num_loss_plateu and loss &gt; 0.1:         last_losses = [loss_dict[i][\"loss\"] for i in range(t-num_loss_plateu, t + 1)]         diffs = [abs(last_losses[i] - last_losses[i - 1]) for i in range(-1, -num_loss_plateu-1, -1)]         if all(diff &lt; min_change for diff in diffs):             # Reset the learning rate to the initial value and recreate the scheduler.             for param_group in optimizer.param_groups:                 param_group['lr'] = initial_lr             scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)         else:             # Update the learning rate.             scheduler.step()     else:         # Update the learning rate.         scheduler.step()      # Enforce constraints on optimizable parameters.     model.check_constraints()      if loss &lt; 0.0009:         print(f\"[t={t}]loss: {float(loss):&gt;7f}\")         break      # Update the sequence with changed pulse parameter values.     model.update_sequence()      if (t % 50 == 0) or (t == epochs-1):         # Print the learning rate.         lr = scheduler.get_last_lr()[0]         print(f\"Epoch {t:03}: Learning Rate = {lr:.6f}\")          print(f\"[t={t}]loss: {float(loss):&gt;7f}\")         print(\"**************************************\")  # Get the best parameter set. sorted_losses = dict(sorted(loss_dict.items(), key=lambda x: x[1][\"loss\"])) best_param_set = list(sorted_losses.values())[0][\"params\"] print(f\"Best loss: {list(sorted_losses.values())[0]['loss']} after {list(sorted_losses.keys())[0]} epochs.\") <pre>Epoch 000: Learning Rate = 0.999013\n[t=0]loss: 0.867522\n**************************************\nEpoch 050: Learning Rate = 0.000987\n[t=50]loss: 0.006605\n**************************************\nEpoch 100: Learning Rate = 0.999013\n[t=100]loss: 0.004576\n**************************************\nEpoch 150: Learning Rate = 0.000987\n[t=150]loss: 0.004507\n**************************************\nEpoch 200: Learning Rate = 0.999013\n[t=200]loss: 0.004503\n**************************************\nEpoch 250: Learning Rate = 0.000987\n[t=250]loss: 0.004502\n**************************************\nEpoch 300: Learning Rate = 0.999013\n[t=300]loss: 0.004502\n**************************************\nEpoch 350: Learning Rate = 0.000987\n[t=350]loss: 0.004502\n**************************************\nEpoch 400: Learning Rate = 0.999013\n[t=400]loss: 0.004748\n**************************************\nEpoch 450: Learning Rate = 0.000987\n[t=450]loss: 0.004565\n**************************************\nEpoch 500: Learning Rate = 0.999013\n[t=500]loss: 0.004512\n**************************************\nEpoch 550: Learning Rate = 0.000987\n[t=550]loss: 0.004502\n**************************************\nEpoch 600: Learning Rate = 0.999013\n[t=600]loss: 0.004987\n**************************************\nEpoch 650: Learning Rate = 0.000987\n[t=650]loss: 0.004517\n**************************************\nEpoch 700: Learning Rate = 0.999013\n[t=700]loss: 0.004513\n**************************************\nEpoch 750: Learning Rate = 0.000987\n[t=750]loss: 0.004511\n**************************************\nEpoch 800: Learning Rate = 0.999013\n[t=800]loss: 0.004510\n**************************************\nEpoch 850: Learning Rate = 0.000987\n[t=850]loss: 0.004509\n**************************************\nEpoch 900: Learning Rate = 0.999013\n[t=900]loss: 0.004509\n**************************************\nEpoch 950: Learning Rate = 0.000987\n[t=950]loss: 0.004508\n**************************************\nEpoch 999: Learning Rate = 1.000000\n[t=999]loss: 0.004507\n**************************************\nBest loss: 0.004501088692131505 after 590 epochs.\n</pre> In\u00a0[17]: Copied! <pre># Update the model params with the best optimized parameter values.\nfor n, p in model.named_parameters():\n    p.data = best_param_set[n]\nmodel.check_constraints()\nmodel.update_sequence()\n</pre> # Update the model params with the best optimized parameter values. for n, p in model.named_parameters():     p.data = best_param_set[n] model.check_constraints() model.update_sequence() <p>Finally, let's plot the pulse sequence and retrieve parameter values.</p> In\u00a0[18]: Copied! <pre>model.built_seq.draw(draw_phase_curve=True)\n\nfor name, param in model.named_parameters():\n    print(name)\n    print(param)\n    print(\"----------------\")\n\n_, gate = model.forward()\nprint(f\"Gate fidelity: {100*(1 - float(gate_infidelity(gate[-1], target_gate))):.2f}%\")\n</pre> model.built_seq.draw(draw_phase_curve=True)  for name, param in model.named_parameters():     print(name)     print(param)     print(\"----------------\")  _, gate = model.forward() print(f\"Gate fidelity: {100*(1 - float(gate_infidelity(gate[-1], target_gate))):.2f}%\")  <pre>seq_param_values.det_param_6\nParameter containing:\ntensor(1.1914, requires_grad=True)\n----------------\nseq_param_values.det_param_7\nParameter containing:\ntensor(1.1712, requires_grad=True)\n----------------\nseq_param_values.phase_param_6\nParameter containing:\ntensor(5.0574, requires_grad=True)\n----------------\nseq_param_values.det_param_4\nParameter containing:\ntensor(2.6439, requires_grad=True)\n----------------\nseq_param_values.amp_param_4\nParameter containing:\ntensor(3.4353, requires_grad=True)\n----------------\nseq_param_values.phase_param_2\nParameter containing:\ntensor(4.2875, requires_grad=True)\n----------------\nseq_param_values.amp_param_1\nParameter containing:\ntensor(0.8496, requires_grad=True)\n----------------\nseq_param_values.phase_param_5\nParameter containing:\ntensor(3.0125, requires_grad=True)\n----------------\nseq_param_values.phase_param_3\nParameter containing:\ntensor(3.9201, requires_grad=True)\n----------------\nseq_param_values.det_param_2\nParameter containing:\ntensor(4.5551, requires_grad=True)\n----------------\nseq_param_values.det_param_5\nParameter containing:\ntensor(2.3943, requires_grad=True)\n----------------\nseq_param_values.amp_param_0\nParameter containing:\ntensor(3.1792, requires_grad=True)\n----------------\nseq_param_values.det_param_3\nParameter containing:\ntensor(2.1601, requires_grad=True)\n----------------\nseq_param_values.det_param_0\nParameter containing:\ntensor(3.4717, requires_grad=True)\n----------------\nseq_param_values.phase_param_4\nParameter containing:\ntensor(3.5054, requires_grad=True)\n----------------\nseq_param_values.phase_param_1\nParameter containing:\ntensor(7.4957, requires_grad=True)\n----------------\nseq_param_values.amp_param_5\nParameter containing:\ntensor(1.6199, requires_grad=True)\n----------------\nseq_param_values.amp_param_3\nParameter containing:\ntensor(3.6246, requires_grad=True)\n----------------\nseq_param_values.amp_param_6\nParameter containing:\ntensor(0., requires_grad=True)\n----------------\nseq_param_values.amp_param_7\nParameter containing:\ntensor(2.0420, requires_grad=True)\n----------------\nseq_param_values.amp_param_2\nParameter containing:\ntensor(1.3270, requires_grad=True)\n----------------\nseq_param_values.det_param_1\nParameter containing:\ntensor(4.9357, requires_grad=True)\n----------------\nseq_param_values.phase_param_7\nParameter containing:\ntensor(5.1786, requires_grad=True)\n----------------\nseq_param_values.phase_param_0\nParameter containing:\ntensor(7.6431, requires_grad=True)\n----------------\nGate fidelity: 99.55%\n</pre> <p>As we can see the fidelity of the simulated Hadamard gate is 99.91%, on par with the results presented in the paper.</p> <p>Ideal constant pulses are not exactly implementable on a real quantum device. The gate optimization problem should generate more realistic, smoother pulse shapes. To achieve this, we sample $N$ points at times $\\{t_n\\}_{n\\in[1,N]}$ representing values of the pulse amplitude $\\Omega_i$ and then sine-interpolate for the missing ones. Generally, in <code>PulserDiff</code>, the complete continuous function $\\Omega(t)$ is not needed, but only discrete samples at $t_k$ where $k \\in \\{1,2,...,T\\}$ with $T$ being the total duration of the sequence. The reason being that the underlying <code>pulser-core</code> engine discretizes any sequence in 1 ns steps.</p> <p>Let us define the interpolation procedure more rigorously. We start from a sample set of values $\\{\\theta_i\\}_{i\\in[n,N]}$ that serve as parameters governing the shape of the resulting waveform. The goal is to construct a function $\\Omega(t)$ such that $\\Omega(t_n) = \\theta_n$. To ensure smoothness and continuity, we use the sine transition function defined as:</p> <p>$$ s(t) = \\frac{1 + \\sin(\\pi t - \\frac{\\pi}{2})}{2}. $$</p> <p>This function transitions smoothly from 0 to 1 as $t \\in [0, 1]$, and has continuous first derivatives. Now, for any time $t \\in [t_k, t_{k+1}]$, we define:</p> <p>$$ \\Omega(t) = \\theta_k (1 - s(h)) + \\theta_{k+1} s(h) $$</p> <p>where $h = \\frac{t - t_k}{t_{k+1} - t_k} \\in [0, 1]$. This ensures that $\\Omega(t)$ is continuously differentiable and satisfies the interpolation condition $\\Omega(t_n) = \\theta_n$. Since $\\Omega(t)$ is a linear combination of parameters $\\theta_n$, the entire function is linearly dependent on the vector of parameters $\\mathbf{\\theta}$. Since any sequence in <code>PulserDiff</code> is discretized in 1 ns steps we can precompute a matrix $A \\in \\mathbb{R}^{T \\times N}$ such that the value of the waveform at a time $t_k$ is given by:</p> <p>$$ \\Omega(t_k) = (\\mathbf{A} \\cdot \\mathbf{\\theta})_k $$</p> <p>where $\\Omega_k$ is the interpolated value at time $t_k$.</p> <p>The algorithm described above is implemented in function <code>interpolate_sine()</code> that can be imported from the <code>pulser_diff.utils</code> module.</p> <p>Let us now explore a slightly more involved example of pulse optimization with a register consisting of 4 qubits and define the target gate as before: the tensor product of single-qubit Hadamard gates.</p> In\u00a0[19]: Copied! <pre># Create a register with 4 qubits.\nn_qubits = 4\ndist = torch.tensor([6.5], requires_grad=False)\nreg = Register.rectangle(1, n_qubits, spacing=dist)\nreg.draw()\n\n# Create the corresponding target Hadamard gate.\ntarget_gate = kron(*[HMAT for _ in range(n_qubits)])\n\n# Create a tensor of batched initial states to get the complete gate matrix on output.\ninit_state = torch.eye(2**n_qubits)\n</pre> # Create a register with 4 qubits. n_qubits = 4 dist = torch.tensor([6.5], requires_grad=False) reg = Register.rectangle(1, n_qubits, spacing=dist) reg.draw()  # Create the corresponding target Hadamard gate. target_gate = kron(*[HMAT for _ in range(n_qubits)])  # Create a tensor of batched initial states to get the complete gate matrix on output. init_state = torch.eye(2**n_qubits) <p>In contrast with the previous case where we used 8 constant pulses in the parametrized sequence, now the sequence contains a single pulse with custom waveforms for the amplitude and detuning defined with Pulser's <code>CustomWaveform</code> object. The interpolation matrix is used in the definition of the functions <code>custom_wf_amp()</code> and <code>custom_wf_det()</code> that are responsible for calculation of the actual values of the corresponding waveforms.</p> <p>Note that in the function <code>custom_wf_amp()</code> a <code>torch.sigmoid()</code> function is used to confine control parameters in the range $[0, \\Omega_{\\rm{max}}]$ where $\\Omega_{\\rm{max}}$ is the maximum amplitude allowed by the device used to create the sequence. Although <code>PulserDiff</code> currently supports constraints on trainable parameters, amplitude values are now calculated from user-supplied trainable parameters and the <code>check_constraints()</code> method in the <code>QuantumModel</code> does not apply in this case.</p> In\u00a0[21]: Copied! <pre># Define sequence parameters.\nduration = 1100\nn_param = 20\ngamma = 0.05\n\n# Create a sequence and declare channels.\nseq = Sequence(reg, MockDevice)\nseq.declare_channel(\"rydberg_global\", \"rydberg_global\")\n\n# Define a custom-shaped pulse.\namp_custom_param = seq.declare_variable(\"amp_custom\", size=duration)\ndet_custom_param = seq.declare_variable(\"det_custom\", size=duration)\ncust_amp = CustomWaveform(amp_custom_param)\ncust_det = CustomWaveform(det_custom_param)\npulse_custom = Pulse(cust_amp, cust_det, 0.0)\n\n# Add pulse to the sequence.\nseq.add(pulse_custom, \"rydberg_global\")\n\n# Create the sine interpolation matrix.\ninterp_mat = interpolate_sine(n_param, duration)\n\n# Define the waveform functions.\ndef custom_wf_amp(params):\n    return torch.matmul(interp_mat, int(MockDevice.channels[\"rydberg_global\"].max_amp) * torch.sigmoid(gamma * params))\n\ndef custom_wf_det(params):\n    return torch.matmul(interp_mat, int(MockDevice.channels[\"rydberg_global\"].max_abs_detuning) * torch.tanh(gamma * params))\n\n# Define the pulse parameters.\namp_values = 5 * torch.rand(n_param, requires_grad=True) - 2.5\ndet_values = 5 * torch.rand(n_param, requires_grad=True) - 2.5\ntrainable_params = {\n    \"amp_custom\": ((amp_values,), custom_wf_amp),\n    \"det_custom\": ((det_values,), custom_wf_det),\n}\n\n# Create a quantum model from the sequence.\nmodel = QuantumModel(seq, trainable_params, sampling_rate=0.05, solver=SolverType.DP5_SE, initial_state=init_state)\n\n# List trainable parameters of the model.\nprint()\nfor name, param in model.named_parameters():\n    print(name)\n    print(param)\n    print('-------')\n\n# Draw the initial sequence.\nmodel.built_seq.draw()\n</pre> # Define sequence parameters. duration = 1100 n_param = 20 gamma = 0.05  # Create a sequence and declare channels. seq = Sequence(reg, MockDevice) seq.declare_channel(\"rydberg_global\", \"rydberg_global\")  # Define a custom-shaped pulse. amp_custom_param = seq.declare_variable(\"amp_custom\", size=duration) det_custom_param = seq.declare_variable(\"det_custom\", size=duration) cust_amp = CustomWaveform(amp_custom_param) cust_det = CustomWaveform(det_custom_param) pulse_custom = Pulse(cust_amp, cust_det, 0.0)  # Add pulse to the sequence. seq.add(pulse_custom, \"rydberg_global\")  # Create the sine interpolation matrix. interp_mat = interpolate_sine(n_param, duration)  # Define the waveform functions. def custom_wf_amp(params):     return torch.matmul(interp_mat, int(MockDevice.channels[\"rydberg_global\"].max_amp) * torch.sigmoid(gamma * params))  def custom_wf_det(params):     return torch.matmul(interp_mat, int(MockDevice.channels[\"rydberg_global\"].max_abs_detuning) * torch.tanh(gamma * params))  # Define the pulse parameters. amp_values = 5 * torch.rand(n_param, requires_grad=True) - 2.5 det_values = 5 * torch.rand(n_param, requires_grad=True) - 2.5 trainable_params = {     \"amp_custom\": ((amp_values,), custom_wf_amp),     \"det_custom\": ((det_values,), custom_wf_det), }  # Create a quantum model from the sequence. model = QuantumModel(seq, trainable_params, sampling_rate=0.05, solver=SolverType.DP5_SE, initial_state=init_state)  # List trainable parameters of the model. print() for name, param in model.named_parameters():     print(name)     print(param)     print('-------')  # Draw the initial sequence. model.built_seq.draw() <pre>\ncall_param_values.amp_custom_0\nParameter containing:\ntensor([-0.6990, -1.1266,  0.2312,  1.7064,  2.3403, -0.6907, -0.5883,  1.0251,\n        -0.4023,  0.2918, -2.0087,  1.2236, -1.6916, -1.1019,  0.8537, -1.5967,\n         0.6686,  1.5748, -1.4093, -0.9600], requires_grad=True)\n-------\ncall_param_values.det_custom_0\nParameter containing:\ntensor([-0.7758,  0.8404,  1.1174, -1.0271, -1.5233, -1.3598, -2.3105,  2.4627,\n         0.5028, -1.3735, -0.1984,  1.9218,  0.3765, -1.7813,  0.8958,  0.2888,\n        -0.0547, -0.8894,  1.7436,  1.4294], requires_grad=True)\n-------\n</pre> <p>We can see from the figure above that the initial waveforms are indeed smooth functions that are governed by $N=20$ parameters each.</p> <p>Let us now define the optimization loop. In this case we use the <code>CosineAnnealingLR</code> learning rate scheduler available in <code>torch</code> to help with the optimization in the more complex loss lansdcape of the 4-qubit system. We also implement a check for loss plateaus: the state of the learning rate scheduler resets to the initial high value when the loss does not decrease rapidly enough. This procedure helps in getting away from plateaus and avoiding local minima. Another measure from standard ML practices is logging the loss value together with the corresponding model parameters. This allows to choose the best set of parameters encountered throughout the scheduled optimization procedure.</p> In\u00a0[22]: Copied! <pre># Define parameter values and initialize the optimizer and the scheduler.\ninitial_lr = 5.0\noptimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\nepochs = 1000\nmin_change = 0.01\nnum_loss_plateu = 6\n\nloss_dict = {}\nfor t in range(epochs):\n    # Calculate the loss with the final state.\n    _, gate = model.forward()\n    loss = gate_infidelity(target_gate, gate[-1])\n    \n    # Backpropagation.\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n    # Log the loss value together with model params.\n    loss_dict[t] = {\"loss\": float(loss), \"params\": {name: param.data.clone().detach() for name, param in model.named_parameters()}}\n\n    if len(loss_dict) &gt; num_loss_plateu and loss &gt; 0.1:\n        last_losses = [loss_dict[i][\"loss\"] for i in range(t-num_loss_plateu, t + 1)]\n        diffs = [abs(last_losses[i] - last_losses[i - 1]) for i in range(-1, -num_loss_plateu-1, -1)]\n        if all(diff &lt; min_change for diff in diffs):\n            # Reset the learning rate to the initial value and recreate the scheduler.\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = initial_lr\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n        else:\n            # Update the learning rate.\n            scheduler.step()\n    else:\n        # Update the learning rate.\n        scheduler.step()\n\n    if loss &lt; 0.0001:\n        print(f\"[t={t}]loss: {float(loss):&gt;7f}\")\n        break\n\n    # Update the sequence with changed pulse parameter values.\n    model.update_sequence()\n\n    if (t % 50 == 0) or (t == epochs-1):\n        # Print the learning rate.\n        lr = scheduler.get_last_lr()[0]\n        print(f\"Epoch {t:03}: Learning Rate = {lr:.6f}\")\n\n        print(f\"[t={t}]loss: {float(loss):&gt;7f}\")\n        print(\"**************************************\")\n\n# Get the best parameter set.\nsorted_losses = dict(sorted(loss_dict.items(), key=lambda x: x[1][\"loss\"]))\nbest_param_set = list(sorted_losses.values())[0][\"params\"]\nprint(f\"Best loss: {list(sorted_losses.values())[0]['loss']} after {list(sorted_losses.keys())[0]} epochs.\")\n</pre> # Define parameter values and initialize the optimizer and the scheduler. initial_lr = 5.0 optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr) scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50) epochs = 1000 min_change = 0.01 num_loss_plateu = 6  loss_dict = {} for t in range(epochs):     # Calculate the loss with the final state.     _, gate = model.forward()     loss = gate_infidelity(target_gate, gate[-1])          # Backpropagation.     loss.backward()     optimizer.step()     optimizer.zero_grad()      # Log the loss value together with model params.     loss_dict[t] = {\"loss\": float(loss), \"params\": {name: param.data.clone().detach() for name, param in model.named_parameters()}}      if len(loss_dict) &gt; num_loss_plateu and loss &gt; 0.1:         last_losses = [loss_dict[i][\"loss\"] for i in range(t-num_loss_plateu, t + 1)]         diffs = [abs(last_losses[i] - last_losses[i - 1]) for i in range(-1, -num_loss_plateu-1, -1)]         if all(diff &lt; min_change for diff in diffs):             # Reset the learning rate to the initial value and recreate the scheduler.             for param_group in optimizer.param_groups:                 param_group['lr'] = initial_lr             scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)         else:             # Update the learning rate.             scheduler.step()     else:         # Update the learning rate.         scheduler.step()      if loss &lt; 0.0001:         print(f\"[t={t}]loss: {float(loss):&gt;7f}\")         break      # Update the sequence with changed pulse parameter values.     model.update_sequence()      if (t % 50 == 0) or (t == epochs-1):         # Print the learning rate.         lr = scheduler.get_last_lr()[0]         print(f\"Epoch {t:03}: Learning Rate = {lr:.6f}\")          print(f\"[t={t}]loss: {float(loss):&gt;7f}\")         print(\"**************************************\")  # Get the best parameter set. sorted_losses = dict(sorted(loss_dict.items(), key=lambda x: x[1][\"loss\"])) best_param_set = list(sorted_losses.values())[0][\"params\"] print(f\"Best loss: {list(sorted_losses.values())[0]['loss']} after {list(sorted_losses.keys())[0]} epochs.\") <pre>Epoch 000: Learning Rate = 4.995067\n[t=0]loss: 0.906707\n**************************************\nEpoch 050: Learning Rate = 0.004933\n[t=50]loss: 0.013457\n**************************************\nEpoch 100: Learning Rate = 4.995067\n[t=100]loss: 0.002192\n**************************************\nEpoch 150: Learning Rate = 0.004933\n[t=150]loss: 0.001843\n**************************************\nEpoch 200: Learning Rate = 4.995067\n[t=200]loss: 0.001782\n**************************************\nEpoch 250: Learning Rate = 0.004933\n[t=250]loss: 0.001730\n**************************************\nEpoch 300: Learning Rate = 4.995067\n[t=300]loss: 0.001687\n**************************************\nEpoch 350: Learning Rate = 0.004933\n[t=350]loss: 0.001650\n**************************************\nEpoch 400: Learning Rate = 4.995067\n[t=400]loss: 0.001620\n**************************************\nEpoch 450: Learning Rate = 0.004933\n[t=450]loss: 0.001594\n**************************************\nEpoch 500: Learning Rate = 4.995067\n[t=500]loss: 0.001575\n**************************************\nEpoch 550: Learning Rate = 0.004933\n[t=550]loss: 0.001556\n**************************************\nEpoch 600: Learning Rate = 4.995067\n[t=600]loss: 0.037805\n**************************************\nEpoch 650: Learning Rate = 0.004933\n[t=650]loss: 0.001564\n**************************************\nEpoch 700: Learning Rate = 4.995067\n[t=700]loss: 0.001530\n**************************************\nEpoch 750: Learning Rate = 0.004933\n[t=750]loss: 0.001518\n**************************************\nEpoch 800: Learning Rate = 4.995067\n[t=800]loss: 0.001511\n**************************************\nEpoch 850: Learning Rate = 0.004933\n[t=850]loss: 0.001505\n**************************************\nEpoch 900: Learning Rate = 4.995067\n[t=900]loss: 0.001499\n**************************************\nEpoch 950: Learning Rate = 0.004933\n[t=950]loss: 0.001494\n**************************************\nEpoch 999: Learning Rate = 5.000000\n[t=999]loss: 0.001490\n**************************************\nBest loss: 0.00148959434305318 after 999 epochs.\n</pre> <p>After the optimization completion, we select the best set of parameters and update the model with these values.</p> In\u00a0[23]: Copied! <pre># update model params with the best optimized parameter values\nfor n, p in model.named_parameters():\n    p.data = best_param_set[n]\nmodel.update_sequence()\n</pre> # update model params with the best optimized parameter values for n, p in model.named_parameters():     p.data = best_param_set[n] model.update_sequence() In\u00a0[24]: Copied! <pre># Draw the optimized custom pulse.\nmodel.built_seq.draw(draw_phase_curve=True)\n\nfor name, param in model.named_parameters():\n    print(name)\n    print(param)\n    print(\"----------------\")\n\n_, gate = model.forward()\nprint(f\"Gate fidelity: {100*(1 - float(gate_infidelity(gate[-1], target_gate))):.2f}%\")\n</pre> # Draw the optimized custom pulse. model.built_seq.draw(draw_phase_curve=True)  for name, param in model.named_parameters():     print(name)     print(param)     print(\"----------------\")  _, gate = model.forward() print(f\"Gate fidelity: {100*(1 - float(gate_infidelity(gate[-1], target_gate))):.2f}%\") <pre>call_param_values.amp_custom_0\nParameter containing:\ntensor([-16.8346, -31.3419, -56.6626, -66.5125, -35.3561, -47.5504, -85.9064,\n        -89.2980, -61.9608,  38.0063,  56.7492, -11.3899, -83.5173, -79.3703,\n        -30.7206, -37.4819, -56.5160, -41.8274, -43.7010, -29.7942],\n       requires_grad=True)\n----------------\ncall_param_values.det_custom_0\nParameter containing:\ntensor([ -7.1687,  17.7202, -28.7036, -34.9884, -36.9152, -37.2416, -36.8077,\n          4.0282,  24.8121,  31.4093,  33.0967,  26.8384,  -6.2463, -38.2686,\n        -37.7148, -38.5522, -38.2611, -35.5587,  12.0700,  13.9825],\n       requires_grad=True)\n----------------\nGate fidelity: 99.85%\n</pre> <p>We can see that the shape of the initial amplitude and detuning waveforms changed drastically to achieve a very respectable 99.84% gate fidelity.</p>"},{"location":"gate_optimization/#gate-optimization","title":"Gate optimization\u00b6","text":""},{"location":"gate_optimization/#optimization-using-constant-pulse-sequences","title":"Optimization using constant-pulse sequences\u00b6","text":""},{"location":"gate_optimization/#optimization-using-custom-waveform-pulse-sequences","title":"Optimization using custom-waveform pulse sequences\u00b6","text":""},{"location":"state_preparation/","title":"State preparation","text":"<p>A vast majority of quantum algorithms (i.e. Quantum Phase Estimation) require to prepare specific initial states to run correctly. In this tutorial, we show how an arbitrary state can be prepared using pulse sequence optimization capabilities of <code>PulserDiff</code>.</p> In\u00a0[\u00a0]: Copied! <pre>import torch\n\nfrom torch import Tensor\nfrom pulser import Sequence, Pulse, Register \nfrom pulser.devices import VirtualDevice\nfrom pulser.waveforms import CustomWaveform\nfrom pulser.channels import Rydberg\n\nfrom pulser_diff.model import QuantumModel\n\nfrom pyqtorch.utils import SolverType\n\nfrom math import pi, sin\nfrom pulser_diff.utils import basis_state, interpolate_sine\n\n\nMockDevice = VirtualDevice(\n    name=\"MockDevice\",\n    dimensions=2,\n    rydberg_level=60,\n    channel_objects=(\n        Rydberg.Global(6.28, 12.566370614359172, max_duration=None),\n    ),\n)\n</pre> import torch  from torch import Tensor from pulser import Sequence, Pulse, Register  from pulser.devices import VirtualDevice from pulser.waveforms import CustomWaveform from pulser.channels import Rydberg  from pulser_diff.model import QuantumModel  from pyqtorch.utils import SolverType  from math import pi, sin from pulser_diff.utils import basis_state, interpolate_sine   MockDevice = VirtualDevice(     name=\"MockDevice\",     dimensions=2,     rydberg_level=60,     channel_objects=(         Rydberg.Global(6.28, 12.566370614359172, max_duration=None),     ), ) <p>Let us first define a 6-qubit register.</p> In\u00a0[4]: Copied! <pre>dist = torch.tensor([7.0])\nn_qubits = 6\nreg = Register.rectangle(1, n_qubits, dist)\nreg.draw()\n</pre> dist = torch.tensor([7.0]) n_qubits = 6 reg = Register.rectangle(1, n_qubits, dist) reg.draw() <p>In this simulation we choose the state $\\left|1...1\\right\\rangle $ as the target state to prepare whereas we initialize the optimization process with the zero state $\\left|0...0\\right\\rangle $. A metric for state similarity is defined as the state fidelity $F=|\\left.\\left\\langle \\psi_{\\rm{final}}\\right| \\psi_{\\rm{target}} \\right\\rangle |^2$ which serves to define the optimization loss as $L=1-F$.</p> In\u00a0[\u00a0]: Copied! <pre># Define the |1...1&gt; target state.\ntarget_state = basis_state(2 ** n_qubits, 0).to(dtype=torch.complex128)\n\ndef fidelity(state1: Tensor, state2: Tensor) -&gt; Tensor:\n    return torch.abs(torch.matmul(state1.mH, state2)) ** 2\n</pre> # Define the |1...1&gt; target state. target_state = basis_state(2 ** n_qubits, 0).to(dtype=torch.complex128)  def fidelity(state1: Tensor, state2: Tensor) -&gt; Tensor:     return torch.abs(torch.matmul(state1.mH, state2)) ** 2 <p>We will use custom-pulse sequence with amplitude and detuning waveforms constructed using sine function interpolation.</p> In\u00a0[26]: Copied! <pre># Define sequence parameters.\nduration = 1100\nn_param = 30\ngamma = 0.02\n\n# Create the sequence and declare the channels.\nseq = Sequence(reg, MockDevice)\nseq.declare_channel(\"rydberg_global\", \"rydberg_global\")\n\n# Define the custom-shaped pulse.\namp_custom_param = seq.declare_variable(\"amp_custom\", size=duration)\ndet_custom_param = seq.declare_variable(\"det_custom\", size=duration)\ncust_amp = CustomWaveform(amp_custom_param)\ncust_det = CustomWaveform(det_custom_param)\npulse_custom = Pulse(cust_amp, cust_det, 0.0)\n\n# Add the pulse.\nseq.add(pulse_custom, \"rydberg_global\")\n\n# Create the sine interpolation matrix.\ninterp_mat = interpolate_sine(n_param, duration)\n\n# Define the waveform functions.\ndef custom_wf_amp(params):\n    return torch.matmul(interp_mat, int(MockDevice.channels[\"rydberg_global\"].max_amp) * torch.sigmoid(gamma * params))\n\ndef custom_wf_det(params):\n    return torch.matmul(interp_mat, int(MockDevice.channels[\"rydberg_global\"].max_abs_detuning) * torch.tanh(gamma * params))\n\n# Define the pulse parameters.\namp_values = 2 * torch.rand(n_param, requires_grad=True) - 1.0\ndet_values = 2 * torch.rand(n_param, requires_grad=True) - 1.0\ntrainable_params = {\n    \"amp_custom\": ((amp_values,), custom_wf_amp),\n    \"det_custom\": ((det_values,), custom_wf_det),\n}\n\n# Create the quantum model from the sequence.\nmodel = QuantumModel(seq, trainable_params, sampling_rate=0.05, solver=SolverType.DP5_SE)\n\n# List all trainable parameters of the model.\nprint()\nfor name, param in model.named_parameters():\n    print(name)\n    print(param)\n    print('-------')\n\nmodel.built_seq.draw()\n</pre> # Define sequence parameters. duration = 1100 n_param = 30 gamma = 0.02  # Create the sequence and declare the channels. seq = Sequence(reg, MockDevice) seq.declare_channel(\"rydberg_global\", \"rydberg_global\")  # Define the custom-shaped pulse. amp_custom_param = seq.declare_variable(\"amp_custom\", size=duration) det_custom_param = seq.declare_variable(\"det_custom\", size=duration) cust_amp = CustomWaveform(amp_custom_param) cust_det = CustomWaveform(det_custom_param) pulse_custom = Pulse(cust_amp, cust_det, 0.0)  # Add the pulse. seq.add(pulse_custom, \"rydberg_global\")  # Create the sine interpolation matrix. interp_mat = interpolate_sine(n_param, duration)  # Define the waveform functions. def custom_wf_amp(params):     return torch.matmul(interp_mat, int(MockDevice.channels[\"rydberg_global\"].max_amp) * torch.sigmoid(gamma * params))  def custom_wf_det(params):     return torch.matmul(interp_mat, int(MockDevice.channels[\"rydberg_global\"].max_abs_detuning) * torch.tanh(gamma * params))  # Define the pulse parameters. amp_values = 2 * torch.rand(n_param, requires_grad=True) - 1.0 det_values = 2 * torch.rand(n_param, requires_grad=True) - 1.0 trainable_params = {     \"amp_custom\": ((amp_values,), custom_wf_amp),     \"det_custom\": ((det_values,), custom_wf_det), }  # Create the quantum model from the sequence. model = QuantumModel(seq, trainable_params, sampling_rate=0.05, solver=SolverType.DP5_SE)  # List all trainable parameters of the model. print() for name, param in model.named_parameters():     print(name)     print(param)     print('-------')  model.built_seq.draw() <pre>\ncall_param_values.amp_custom_0\nParameter containing:\ntensor([ 0.1710, -0.6696, -0.6378, -0.2017, -0.4131,  0.6563, -0.8746,  0.7353,\n        -0.9874,  0.3153,  0.8834,  0.8186,  0.4730,  0.0033,  0.2423, -0.1764,\n         0.3488, -0.6039,  0.0627, -0.5089, -0.6097, -0.6837, -0.0811,  0.5327,\n        -0.6679,  0.1670,  0.5462,  0.4381,  0.3342,  0.2942],\n       requires_grad=True)\n-------\ncall_param_values.det_custom_0\nParameter containing:\ntensor([-0.1795, -0.0115, -0.5285, -0.9943,  0.7078,  0.9741, -0.4146,  0.3350,\n         0.4035,  0.3226,  0.4535,  0.6059,  0.8024, -0.5279, -0.4816, -0.8822,\n         0.7213, -0.8643,  0.1060,  0.3222,  0.2893,  0.5684, -0.7081,  0.6178,\n         0.3030, -0.3070,  0.1151,  0.8919,  0.3060, -0.1755],\n       requires_grad=True)\n-------\n</pre> <p>Finally, we run the optimization loop and record the best parameter set for the lowest loss.</p> In\u00a0[27]: Copied! <pre># Define parameter values and initialize the optimizer and the scheduler.\ninitial_lr = 5.0\noptimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\nepochs = 1000\nmin_change = 0.01\nnum_loss_plateu = 6\n\nloss_dict = {}\nfor t in range(epochs):\n    # Calculate the loss with the final state.\n    _, final_state = model.forward()\n    loss = 1 - fidelity(target_state, final_state[-1])\n    \n    # Backpropagation.\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n    # Log the loss value together with model params.\n    loss_dict[t] = {\"loss\": float(loss), \"params\": {name: param.data.clone().detach() for name, param in model.named_parameters()}}\n\n    if len(loss_dict) &gt; num_loss_plateu and loss &gt; 0.1:\n        last_losses = [loss_dict[i][\"loss\"] for i in range(t-num_loss_plateu, t + 1)]\n        diffs = [abs(last_losses[i] - last_losses[i - 1]) for i in range(-1, -num_loss_plateu-1, -1)]\n        if all(diff &lt; min_change for diff in diffs):\n            # Reset the learning rate to the initial value and recreate the scheduler.\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = initial_lr\n            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n        else:\n            # Update the learning rate.\n            scheduler.step()\n    else:\n        # Update the learning rate.\n        scheduler.step()\n\n    if loss &lt; 0.0001:\n        print(f\"[t={t}]loss: {float(loss):&gt;7f}\")\n        break\n\n    # Update the sequence with changed pulse parameter values.\n    model.update_sequence()\n\n    if (t % 50 == 0) or (t == epochs-1):\n        # Print the learning rate.\n        lr = scheduler.get_last_lr()[0]\n        print(f\"Epoch {t:03}: Learning Rate = {lr:.6f}\")\n\n        print(f\"[t={t}]loss: {float(loss):&gt;7f}\")\n        print(\"**************************************\")\n\n# Get the best parameter set.\nsorted_losses = dict(sorted(loss_dict.items(), key=lambda x: x[1][\"loss\"]))\nbest_param_set = list(sorted_losses.values())[0][\"params\"]\nprint(f\"Best loss: {list(sorted_losses.values())[0]['loss']} after {list(sorted_losses.keys())[0]} epochs.\")\n</pre> # Define parameter values and initialize the optimizer and the scheduler. initial_lr = 5.0 optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr) scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50) epochs = 1000 min_change = 0.01 num_loss_plateu = 6  loss_dict = {} for t in range(epochs):     # Calculate the loss with the final state.     _, final_state = model.forward()     loss = 1 - fidelity(target_state, final_state[-1])          # Backpropagation.     loss.backward()     optimizer.step()     optimizer.zero_grad()      # Log the loss value together with model params.     loss_dict[t] = {\"loss\": float(loss), \"params\": {name: param.data.clone().detach() for name, param in model.named_parameters()}}      if len(loss_dict) &gt; num_loss_plateu and loss &gt; 0.1:         last_losses = [loss_dict[i][\"loss\"] for i in range(t-num_loss_plateu, t + 1)]         diffs = [abs(last_losses[i] - last_losses[i - 1]) for i in range(-1, -num_loss_plateu-1, -1)]         if all(diff &lt; min_change for diff in diffs):             # Reset the learning rate to the initial value and recreate the scheduler.             for param_group in optimizer.param_groups:                 param_group['lr'] = initial_lr             scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)         else:             # Update the learning rate.             scheduler.step()     else:         # Update the learning rate.         scheduler.step()      if loss &lt; 0.0001:         print(f\"[t={t}]loss: {float(loss):&gt;7f}\")         break      # Update the sequence with changed pulse parameter values.     model.update_sequence()      if (t % 50 == 0) or (t == epochs-1):         # Print the learning rate.         lr = scheduler.get_last_lr()[0]         print(f\"Epoch {t:03}: Learning Rate = {lr:.6f}\")          print(f\"[t={t}]loss: {float(loss):&gt;7f}\")         print(\"**************************************\")  # Get the best parameter set. sorted_losses = dict(sorted(loss_dict.items(), key=lambda x: x[1][\"loss\"])) best_param_set = list(sorted_losses.values())[0][\"params\"] print(f\"Best loss: {list(sorted_losses.values())[0]['loss']} after {list(sorted_losses.keys())[0]} epochs.\") <pre>Epoch 000: Learning Rate = 4.995067\n[t=0]loss: 0.999458\n**************************************\nEpoch 050: Learning Rate = 4.921458\n[t=50]loss: 0.079615\n**************************************\nEpoch 100: Learning Rate = 0.078542\n[t=100]loss: 0.040372\n**************************************\nEpoch 150: Learning Rate = 4.921458\n[t=150]loss: 0.021692\n**************************************\nEpoch 200: Learning Rate = 0.078542\n[t=200]loss: 0.014938\n**************************************\nEpoch 250: Learning Rate = 4.921458\n[t=250]loss: 0.010457\n**************************************\nEpoch 300: Learning Rate = 0.078542\n[t=300]loss: 0.008373\n**************************************\nEpoch 350: Learning Rate = 4.921458\n[t=350]loss: 0.006738\n**************************************\nEpoch 400: Learning Rate = 0.078542\n[t=400]loss: 0.005820\n**************************************\nEpoch 450: Learning Rate = 4.921458\n[t=450]loss: 0.004981\n**************************************\nEpoch 500: Learning Rate = 0.078542\n[t=500]loss: 0.004457\n**************************************\nEpoch 550: Learning Rate = 4.921458\n[t=550]loss: 0.003941\n**************************************\nEpoch 600: Learning Rate = 0.078542\n[t=600]loss: 0.003601\n**************************************\nEpoch 650: Learning Rate = 4.921458\n[t=650]loss: 0.003255\n**************************************\nEpoch 700: Learning Rate = 0.078542\n[t=700]loss: 0.003021\n**************************************\nEpoch 750: Learning Rate = 4.921458\n[t=750]loss: 0.002778\n**************************************\nEpoch 800: Learning Rate = 0.078542\n[t=800]loss: 0.002608\n**************************************\nEpoch 850: Learning Rate = 4.921458\n[t=850]loss: 0.002429\n**************************************\nEpoch 900: Learning Rate = 0.078542\n[t=900]loss: 0.002303\n**************************************\nEpoch 950: Learning Rate = 4.921458\n[t=950]loss: 0.002168\n**************************************\nEpoch 999: Learning Rate = 0.044282\n[t=999]loss: 0.002072\n**************************************\nBest loss: 0.0020715844334903144 after 999 epochs.\n</pre> In\u00a0[28]: Copied! <pre># update model params with the best optimized parameter values\nfor n, p in model.named_parameters():\n    p.data = best_param_set[n]\nmodel.update_sequence()\n</pre> # update model params with the best optimized parameter values for n, p in model.named_parameters():     p.data = best_param_set[n] model.update_sequence() In\u00a0[29]: Copied! <pre># visualize the optimized custom pulse\nmodel.built_seq.draw(draw_phase_curve=True)\n\nfor name, param in model.named_parameters():\n    print(name)\n    print(param)\n    print(\"----------------\")\n\n_, final_state = model.forward()\nprint(f\"State fidelity: {100 * float(fidelity(final_state[-1], target_state)):.2f}%\")\n</pre> # visualize the optimized custom pulse model.built_seq.draw(draw_phase_curve=True)  for name, param in model.named_parameters():     print(name)     print(param)     print(\"----------------\")  _, final_state = model.forward() print(f\"State fidelity: {100 * float(fidelity(final_state[-1], target_state)):.2f}%\") <pre>call_param_values.amp_custom_0\nParameter containing:\ntensor([-109.8368,   92.4695,  202.7724,  230.8793,  213.1587,  132.3215,\n        -128.9406, -194.4135, -265.8430, -296.8776, -283.3639, -256.2733,\n        -212.3780, -159.6839,  -97.8577, -124.3773, -203.9898, -251.4854,\n        -243.6192, -227.4387, -212.6938, -210.1281, -223.0002, -209.8238,\n        -152.4837,  -76.1234,   45.9807,  106.1897,  115.1693, -127.7764],\n       requires_grad=True)\n----------------\ncall_param_values.det_custom_0\nParameter containing:\ntensor([  28.8744,  -78.7406,   40.0382,  116.4371,  133.6737,  131.5698,\n         103.3717,  -36.8735,  -91.6051, -108.5487, -123.9467, -126.2932,\n         -90.3078,  -74.1485,  -56.0920,  -41.9021,  -52.8627,  -65.9430,\n         -54.2068,  -54.3430,   20.4778,   83.7686,   98.0866,  105.4014,\n         122.9699,  135.6744,  141.4354,  145.4670,  142.7416,  134.6813],\n       requires_grad=True)\n----------------\nState fidelity: 99.79%\n</pre> <p>We can see that that executing the pulse with the optimized custom waveform displayed above results in the transformation $\\left|0...0 \\right \\rangle \\rightarrow \\left|1...1\\right\\rangle $ with 99.99% accuracy in the presence of inter-qubit interaction.</p>"},{"location":"state_preparation/#state-preparation","title":"State preparation\u00b6","text":""}]}